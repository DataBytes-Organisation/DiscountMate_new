{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892aac16-7868-47bd-8967-18de45bb3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")  # Or \"Qt5Agg\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from datetime import timedelta\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03ad821-e66e-4065-ab04-53228ec7d2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Volatile Products by Price:\n",
      "                              product_id  price_volatility\n",
      "17  d7320e1a-eacb-4858-bd84-ef191945c0d7          5.944002\n",
      "1   3b3e2c46-9899-4f14-8f8f-74ec3a1d31a7          5.338124\n",
      "7   5fdf1c13-c0b2-44d8-b073-2cc4384a802f          5.066800\n",
      "10  9e6663e2-5f4d-43c6-ab9a-39ea51aaa01e          4.531573\n",
      "22  feaf28a3-4c63-40e2-9b33-a5a00273f91b          4.449070\n",
      "21  f596dfea-1a5b-4173-aa60-1fbad5af36da          4.408462\n",
      "0   1f59c631-ca0d-47d9-aac2-8da2dbc1f134          4.243929\n",
      "6   5875ebb3-bba7-4356-bb0e-9f3c7679c975          3.866097\n",
      "18  e1f5482e-57ff-4333-aa1d-e0df4a8f1901          3.761193\n",
      "5   560b49be-32ad-4267-97da-21148ccce35e          3.724393\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Prepare Data ---\n",
    "df = pd.read_csv('AugmentedData.product_pricing_full_year_5day.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# --- Price Volatility Analysis ---\n",
    "volatility_df = df.groupby('product_id')['price'].std().reset_index(name='price_volatility')\n",
    "top_volatile = volatility_df.sort_values(by='price_volatility', ascending=False).head(10)\n",
    "print(\"\\nTop 10 Most Volatile Products by Price:\")\n",
    "print(top_volatile)\n",
    "\n",
    "# --- Select Most Common Product for Detailed Analysis ---\n",
    "most_common_product = df['product_id'].value_counts().idxmax()\n",
    "product_df = df[df['product_id'] == most_common_product].copy()\n",
    "product_df = product_df.sort_values('date')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0c26bb-a752-40e5-8cab-ecc20c7ae6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of sales detected for Product 1f59c631-ca0d-47d9-aac2-8da2dbc1f134: 34\n",
      "Average interval between sales: 10 days\n",
      "Last sale date: 2026-03-17\n",
      "Predicted next sale date: 2026-03-27\n"
     ]
    }
   ],
   "source": [
    "# --- Price Trend Visualization ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(product_df['date'], product_df['price'], marker='o', linestyle='-', color='blue')\n",
    "plt.title(f\"Price Trend Over Time – Product ID: {most_common_product}\", fontsize=14)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Price ($)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Sales Detection: Identify Price Drops ---\n",
    "product_df['price_diff'] = product_df['price'].diff()\n",
    "sales_df = product_df[product_df['price_diff'] < 0].copy()\n",
    "num_sales_detected = sales_df.shape[0]\n",
    "print(f\"\\nNumber of sales detected for Product {most_common_product}: {num_sales_detected}\")\n",
    "\n",
    "# --- Time Between Sales and Next Sale Prediction ---\n",
    "sales_df['days_between_sales'] = sales_df['date'].diff().dt.days\n",
    "avg_days_between_sales = int(sales_df['days_between_sales'].mean())\n",
    "last_sale_date = sales_df['date'].max()\n",
    "predicted_next_sale = last_sale_date + timedelta(days=avg_days_between_sales)\n",
    "\n",
    "print(f\"Average interval between sales: {avg_days_between_sales} days\")\n",
    "print(f\"Last sale date: {last_sale_date.date()}\")\n",
    "print(f\"Predicted next sale date: {predicted_next_sale.date()}\")\n",
    "\n",
    "# --- Seasonal Sales Pattern: Monthly Frequency ---\n",
    "sales_df['month'] = sales_df['date'].dt.month\n",
    "monthly_counts = sales_df['month'].value_counts().sort_index()\n",
    "\n",
    "if not monthly_counts.empty:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    monthly_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Monthly Sales Frequency – Product ID: {most_common_product}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Number of Sales\")\n",
    "    plt.xticks(ticks=range(1,13), labels=[\n",
    "        \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n",
    "        \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n",
    "    ], rotation=0)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sales detected for monthly frequency plot.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f01ba7-9635-48ea-8974-4fe89c253e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sales by Weekday ---\n",
    "sales_df['weekday'] = sales_df['date'].dt.day_name()\n",
    "weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "weekday_counts = sales_df['weekday'].value_counts().reindex(weekday_order).fillna(0)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "weekday_counts.plot(kind='bar', color='coral', edgecolor='black')\n",
    "plt.title(f\"Sales by Day of the Week – Product ID: {most_common_product}\")\n",
    "plt.xlabel(\"Weekday\")\n",
    "plt.ylabel(\"Number of Sales\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585591d6-a579-49c6-84da-826aaa2f8023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Distribution of Days Between Sales ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sales_df['days_between_sales'].dropna().hist(bins=10, edgecolor='black', color='olive')\n",
    "plt.title(f\"Distribution of Days Between Sales – Product ID: {most_common_product}\")\n",
    "plt.xlabel(\"Days Between Sales\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Sales Timing Around EOFY (June 30, 2025) ---\n",
    "eofy_date = pd.Timestamp(\"2025-06-30\")\n",
    "sales_df['days_from_eofy'] = (sales_df['date'] - eofy_date).dt.days\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(sales_df['days_from_eofy'], bins=20, edgecolor='black', color='purple')\n",
    "plt.title(f\"Sales Timing Around EOFY – Product ID: {most_common_product}\")\n",
    "plt.xlabel(\"Days From EOFY\")\n",
    "plt.ylabel(\"Number of Sales\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Monthly Sale Drop Magnitude Boxplot ---\n",
    "sales_df['drop_amount'] = -sales_df['price_diff']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sales_df.boxplot(column='drop_amount', by='month', grid=False)\n",
    "plt.title(f\"Monthly Sale Drop Magnitude – Product ID: {most_common_product}\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Price Drop Amount\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9aaccf4-247b-4327-bbf7-221d1d3256b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PED] Skipped: 'quantity' column not present in dataset.\n",
      "Records: 1679 | Products: 23 | Date range: 2025-04-01 00:00:00 → 2026-03-27 00:00:00\n",
      "Saved per-product stats → outlier_full_year_results\\per_product_stats.csv\n",
      "Saved outliers → outlier_full_year_results\\outliers_static_baseline.csv (rows: 101)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abin Baby\\AppData\\Local\\Temp\\ipykernel_9372\\2064087790.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  product_stats = df.groupby(\"product_id\").apply(compute_product_stats).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved hot dates (outliers) → outlier_full_year_results\\hot_dates_outliers.csv\n",
      "Saved multi-page PDF: outlier_full_year_results\\product_trends.pdf\n"
     ]
    }
   ],
   "source": [
    "# --- Top 3 Products Price Trend Comparison ---\n",
    "top3_products = df['product_id'].value_counts().head(3).index.tolist()\n",
    "\n",
    "if top3_products:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for pid in top3_products:\n",
    "        prod_data = df[df['product_id'] == pid].sort_values('date')\n",
    "        plt.plot(prod_data['date'], prod_data['price'], marker='o', linestyle='--', label=f'Product {str(pid)[:6]}')\n",
    "    plt.title(\"Price Comparison for Top 3 Products\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Price ($)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Top-3 trend plot skipped: Not enough products.\")\n",
    "\n",
    "# --- Optional: Price Elasticity of Demand (PED) ---\n",
    "if 'quantity' in df.columns:\n",
    "    try:\n",
    "        ped_df = df[(df['price'] > 0) & (df['quantity'] > 0)].copy()\n",
    "        ped_df['log_price'] = np.log(ped_df['price'])\n",
    "        ped_df['log_qty'] = np.log(ped_df['quantity'])\n",
    "\n",
    "        def ped_slope(group):\n",
    "            x = group['log_price'].values\n",
    "            y = group['log_qty'].values\n",
    "            if len(x) < 2 or np.var(x) == 0:\n",
    "                return np.nan\n",
    "            return np.cov(x, y, ddof=0)[0, 1] / np.var(x)\n",
    "\n",
    "        ped_results = ped_df.groupby('product_id').apply(ped_slope).reset_index(name='ped_slope').dropna()\n",
    "\n",
    "        ped_results['interpretation'] = np.where(\n",
    "            ped_results['ped_slope'] <= -1, 'elastic (price-sensitive)',\n",
    "            np.where(ped_results['ped_slope'] < 0, 'inelastic (price-insensitive)', 'no relationship')\n",
    "        )\n",
    "\n",
    "        print(\"\\n[PED] Estimated Elasticity (dlnQ/dlnP) by Product (most elastic first):\")\n",
    "        print(ped_results.sort_values('ped_slope').head(10))\n",
    "\n",
    "        top_elastic = ped_results.sort_values('ped_slope').head(5)\n",
    "        if not top_elastic.empty:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.barh(top_elastic['product_id'].astype(str).str[:8], top_elastic['ped_slope'])\n",
    "            plt.title(\"Top 5 Most Elastic Products (more negative = more sensitive)\")\n",
    "            plt.xlabel(\"PED (Slope dlnQ/dlnP)\")\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"[PED] Could not compute elasticity: {e}\")\n",
    "else:\n",
    "    print(\"[PED] Skipped: 'quantity' column not present in dataset.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Config\n",
    "# =========================\n",
    "FILE_PATH = r\"AugmentedData.product_pricing_full_year_5day.csv\"\n",
    "OUTPUT_DIR = os.path.join(os.path.dirname(FILE_PATH), \"outlier_full_year_results\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Choose method: \"iqr\", \"zscore\", or \"both\"\n",
    "OUTLIER_METHOD = \"both\"       # robust default\n",
    "IQR_K = 1.5                   # 1.5 (moderate) or 3.0 (strict)\n",
    "ZSCORE_THRESHOLD = 3.0        # classic z-score cut\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Load and clean\n",
    "# =========================\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.rename(columns={\n",
    "        \"_id\": \"_id\",\n",
    "        \"product_id\": \"product_id\",\n",
    "        \"date\": \"date\",\n",
    "        \"price\": \"price\"\n",
    "    })\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "    df[\"price\"] = pd.to_numeric(df[\"price\"].astype(str).str.replace(\"$\", \"\", regex=False), errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"product_id\", \"date\", \"price\"])\n",
    "    df = df[df[\"price\"] > 0]\n",
    "    df = df.sort_values([\"product_id\", \"date\"]).reset_index(drop=True)\n",
    "    df[\"day\"] = df[\"date\"].dt.date\n",
    "    return df\n",
    "\n",
    "df = load_data(FILE_PATH)\n",
    "print(f\"Records: {len(df)} | Products: {df['product_id'].nunique()} | \"\n",
    "      f\"Date range: {df['date'].min()} → {df['date'].max()}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Per-product static stats\n",
    "# =========================\n",
    "def compute_product_stats(g: pd.DataFrame) -> pd.Series:\n",
    "    median_price = g[\"price\"].median()\n",
    "    mean_price   = g[\"price\"].mean()\n",
    "    std_price    = g[\"price\"].std(ddof=1)\n",
    "    q1, q3       = g[\"price\"].quantile([0.25, 0.75])\n",
    "    iqr          = q3 - q1\n",
    "    mad          = np.median(np.abs(g[\"price\"] - median_price))\n",
    "    min_price    = g[\"price\"].min()\n",
    "    max_price    = g[\"price\"].max()\n",
    "    return pd.Series({\n",
    "        \"n_points\": len(g),\n",
    "        \"mean_price\": mean_price,\n",
    "        \"median_price\": median_price,\n",
    "        \"std_price\": std_price if np.isfinite(std_price) else np.nan,\n",
    "        \"q1\": q1,\n",
    "        \"q3\": q3,\n",
    "        \"iqr\": iqr,\n",
    "        \"mad\": mad,\n",
    "        \"min_price\": min_price,\n",
    "        \"max_price\": max_price\n",
    "    })\n",
    "\n",
    "product_stats = df.groupby(\"product_id\").apply(compute_product_stats).reset_index()\n",
    "product_stats.to_csv(os.path.join(OUTPUT_DIR, \"per_product_stats.csv\"), index=False)\n",
    "print(f\"Saved per-product stats → {os.path.join(OUTPUT_DIR, 'per_product_stats.csv')}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Outlier detection (static baseline)\n",
    "# =========================\n",
    "def detect_outliers(df: pd.DataFrame,\n",
    "                    stats: pd.DataFrame,\n",
    "                    method: str = \"iqr\",\n",
    "                    iqr_k: float = 1.5,\n",
    "                    z_thresh: float = 3.0) -> pd.DataFrame:\n",
    "    stats_map = stats.set_index(\"product_id\").to_dict(orient=\"index\")\n",
    "\n",
    "    out = df.copy()\n",
    "    # Attach static baselines and extrema\n",
    "    out[\"median_price\"] = out[\"product_id\"].map(lambda pid: stats_map[pid][\"median_price\"])\n",
    "    out[\"mean_price\"]   = out[\"product_id\"].map(lambda pid: stats_map[pid][\"mean_price\"])\n",
    "    out[\"std_price\"]    = out[\"product_id\"].map(lambda pid: stats_map[pid][\"std_price\"])\n",
    "    out[\"q1\"]           = out[\"product_id\"].map(lambda pid: stats_map[pid][\"q1\"])\n",
    "    out[\"q3\"]           = out[\"product_id\"].map(lambda pid: stats_map[pid][\"q3\"])\n",
    "    out[\"iqr\"]          = out[\"product_id\"].map(lambda pid: stats_map[pid][\"iqr\"])\n",
    "    out[\"min_price\"]    = out[\"product_id\"].map(lambda pid: stats_map[pid][\"min_price\"])\n",
    "    out[\"max_price\"]    = out[\"product_id\"].map(lambda pid: stats_map[pid][\"max_price\"])\n",
    "\n",
    "    # Boundaries for IQR\n",
    "    out[\"iqr_lower\"] = out[\"q1\"] - iqr_k * out[\"iqr\"]\n",
    "    out[\"iqr_upper\"] = out[\"q3\"] + iqr_k * out[\"iqr\"]\n",
    "\n",
    "    # Z-score (used for method \"zscore\" or \"both\")\n",
    "    out[\"z_score\"] = (out[\"price\"] - out[\"mean_price\"]) / out[\"std_price\"]\n",
    "    out.loc[~np.isfinite(out[\"z_score\"]), \"z_score\"] = np.nan\n",
    "\n",
    "    # Flags\n",
    "    iqr_flag = (out[\"price\"] < out[\"iqr_lower\"]) | (out[\"price\"] > out[\"iqr_upper\"])\n",
    "    z_flag   = out[\"z_score\"].abs() > z_thresh\n",
    "\n",
    "    if method == \"iqr\":\n",
    "        out[\"is_outlier\"] = iqr_flag\n",
    "        out[\"outlier_method\"] = \"iqr\"\n",
    "    elif method == \"zscore\":\n",
    "        out[\"is_outlier\"] = z_flag\n",
    "        out[\"outlier_method\"] = \"zscore\"\n",
    "    else:  # both\n",
    "        out[\"is_outlier\"] = iqr_flag | z_flag\n",
    "        out[\"outlier_method\"] = np.where(iqr_flag & z_flag, \"both\",\n",
    "                                  np.where(iqr_flag, \"iqr\", np.where(z_flag, \"zscore\", \"\")))\n",
    "    return out\n",
    "\n",
    "flagged = detect_outliers(df, product_stats, method=OUTLIER_METHOD, iqr_k=IQR_K, z_thresh=ZSCORE_THRESHOLD)\n",
    "\n",
    "outliers = flagged[flagged[\"is_outlier\"]].copy()\n",
    "out_cols = [\n",
    "    \"_id\", \"product_id\", \"date\", \"day\", \"price\",\n",
    "    \"median_price\", \"mean_price\", \"min_price\", \"max_price\",\n",
    "    \"q1\", \"q3\", \"iqr\", \"iqr_lower\", \"iqr_upper\",\n",
    "    \"z_score\", \"outlier_method\"\n",
    "]\n",
    "outliers = outliers[out_cols].sort_values([\"product_id\", \"date\"])\n",
    "outliers_path = os.path.join(OUTPUT_DIR, \"outliers_static_baseline.csv\")\n",
    "outliers.to_csv(outliers_path, index=False)\n",
    "print(f\"Saved outliers → {outliers_path} (rows: {len(outliers)})\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) “Hot dates” analysis (outlier counts only)\n",
    "# =========================\n",
    "hot_outlier = (outliers.groupby(\"day\")[\"product_id\"]\n",
    "               .nunique()\n",
    "               .reset_index(name=\"num_products_outlier\"))\n",
    "hot_outlier[\"share_of_all_products\"] = hot_outlier[\"num_products_outlier\"] / df[\"product_id\"].nunique()\n",
    "hot_outlier = hot_outlier.sort_values(\"num_products_outlier\", ascending=False)\n",
    "hot_outlier_path = os.path.join(OUTPUT_DIR, \"hot_dates_outliers.csv\")\n",
    "hot_outlier.to_csv(hot_outlier_path, index=False)\n",
    "print(f\"Saved hot dates (outliers) → {hot_outlier_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Per-product plots\n",
    "# =========================\n",
    "def plot_product_static_baseline(product_id: str):\n",
    "    g = flagged[flagged[\"product_id\"] == product_id].sort_values(\"date\")\n",
    "    if g.empty:\n",
    "        print(f\"No data for product {product_id}\")\n",
    "        return\n",
    "    med = g[\"median_price\"].iloc[0]\n",
    "    g_out = g[g[\"is_outlier\"]]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(g[\"date\"], g[\"price\"], label=\"Price\", color=\"#1f77b4\")\n",
    "    plt.axhline(med, color=\"#ff7f0e\", linestyle=\"--\", label=\"Median (static baseline)\")\n",
    "    if not g_out.empty:\n",
    "        plt.scatter(g_out[\"date\"], g_out[\"price\"], color=\"red\", s=30, label=\"Outliers\")\n",
    "    plt.title(f\"Product {product_id} - Price vs Static Median\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Price\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    path = os.path.join(OUTPUT_DIR, f\"series_static_{product_id}.png\")\n",
    "    plt.savefig(path, dpi=160); plt.show(block=True)\n",
    "\n",
    "    print(f\"Saved product series → {path}\")\n",
    "\n",
    "# Example:\n",
    "# plot_product_static_baseline(df['product_id'].iloc[0])\n",
    "# plot_product_static_baseline('your-product-id-here')\n",
    "    \n",
    "# create PDF for each product's price trend\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "pdf_path = os.path.join(OUTPUT_DIR, \"product_trends.pdf\")\n",
    "with PdfPages(pdf_path) as pdf:\n",
    "    for pid in df[\"product_id\"].unique():\n",
    "        g = flagged[flagged[\"product_id\"] == pid].sort_values(\"date\")\n",
    "        if g.empty:\n",
    "            continue\n",
    "\n",
    "        med  = g[\"median_price\"].iloc[0]\n",
    "        mean = g[\"mean_price\"].iloc[0]\n",
    "        q1   = g[\"q1\"].iloc[0]\n",
    "        q3   = g[\"q3\"].iloc[0]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 4))\n",
    "        ax.plot(g[\"date\"], g[\"price\"], color=\"#1f77b4\", linewidth=1.5, label=\"Price\")\n",
    "        ax.scatter(g[\"date\"], g[\"price\"], color=\"#1f77b4\", s=10, alpha=0.6)\n",
    "\n",
    "        ax.axhline(med,  color=\"#ff7f0e\", linestyle=\"--\", linewidth=1.2, label=\"Median\")\n",
    "        ax.axhline(mean, color=\"#2ca02c\", linestyle=\"-.\",  linewidth=1.2, label=\"Mean\")\n",
    "        ax.fill_between([g[\"date\"].min(), g[\"date\"].max()], q1, q3, color=\"#ff7f0e\", alpha=0.15, label=\"IQR band\")\n",
    "\n",
    "        out = g[g[\"is_outlier\"]]\n",
    "        if not out.empty:\n",
    "            ax.scatter(out[\"date\"], out[\"price\"], color=\"red\", s=30, zorder=3, label=\"Outlier\")\n",
    "\n",
    "        ax.set_title(f\"Product {pid} — Price vs Static Baseline\")\n",
    "        ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Price\")\n",
    "        ax.legend(loc=\"best\")\n",
    "        fig.autofmt_xdate()\n",
    "        fig.tight_layout()\n",
    "\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "print(f\"Saved multi-page PDF: {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f68631-66d1-44d0-875a-0c3e2b9439a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved product seasonality index → outputs\\seasonality\\product_seasonality_index.csv\n",
      "Saved heatmap → outputs\\seasonality\\product_seasonality_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Product-Level Seasonality Indices\n",
    "# =========================\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure output dir\n",
    "seasonality_dir = os.path.join(os.path.dirname(FILE_PATH), \"outputs\", \"seasonality\")\n",
    "os.makedirs(seasonality_dir, exist_ok=True)\n",
    "\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "# Compute monthly mean price per product\n",
    "monthly_avg = (\n",
    "    df.groupby([\"product_id\", \"month\"])[\"price\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Compute each product's long-run mean\n",
    "long_run_mean = (\n",
    "    df.groupby(\"product_id\")[\"price\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"price\": \"long_run_mean\"})\n",
    ")\n",
    "\n",
    "# Merge and compute index\n",
    "monthly_avg = monthly_avg.merge(long_run_mean, on=\"product_id\")\n",
    "monthly_avg[\"seasonality_index\"] = monthly_avg[\"price\"] / monthly_avg[\"long_run_mean\"]\n",
    "\n",
    "# Save CSV\n",
    "out_csv = os.path.join(seasonality_dir, \"product_seasonality_index.csv\")\n",
    "monthly_avg.to_csv(out_csv, index=False)\n",
    "print(f\"Saved product seasonality index → {out_csv}\")\n",
    "\n",
    "# Pivot for heatmap (products vs months)\n",
    "pivot_df = monthly_avg.pivot(index=\"product_id\", columns=\"month\", values=\"seasonality_index\")\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(pivot_df, cmap=\"coolwarm\", center=1, annot=False, cbar_kws={'label': 'Seasonality Index'})\n",
    "plt.title(\"Product-Level Seasonality Indices (relative to long-run mean)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Product ID\")\n",
    "plt.tight_layout()\n",
    "\n",
    "heatmap_path = os.path.join(seasonality_dir, \"product_seasonality_heatmap.png\")\n",
    "plt.savefig(heatmap_path, dpi=160)\n",
    "plt.show()\n",
    "print(f\"Saved heatmap → {heatmap_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7e929de-b6ae-416b-b65b-ab89f627dd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Seasonal Peaks (product_id, month, index):\n",
      "                               product_id  month  seasonality_index\n",
      "130  9e6663e2-5f4d-43c6-ab9a-39ea51aaa01e     11           1.134086\n",
      "143  b9935dad-617a-4bf2-9dff-5f01faf98355     12           1.122108\n",
      "83   5875ebb3-bba7-4356-bb0e-9f3c7679c975     12           1.121273\n",
      "11   1f59c631-ca0d-47d9-aac2-8da2dbc1f134     12           1.119508\n",
      "215  d7320e1a-eacb-4858-bd84-ef191945c0d7     12           1.117290\n",
      "\n",
      "Top 5 Seasonal Dips (product_id, month, index):\n",
      "                               product_id  month  seasonality_index\n",
      "49   54725161-52ca-4b2e-8df7-0dcd9d427adf      2           0.857715\n",
      "240  e79a6d75-9fed-4859-a342-cedc1fa5a132      1           0.871649\n",
      "25   485e84e6-5885-4369-9073-27e732b0c01d      2           0.875249\n",
      "253  f596dfea-1a5b-4173-aa60-1fbad5af36da      2           0.882849\n",
      "120  9e6663e2-5f4d-43c6-ab9a-39ea51aaa01e      1           0.885521\n",
      "Saved top peaks → outputs\\seasonality\\product_seasonality_peaks.csv\n",
      "Saved top dips → outputs\\seasonality\\product_seasonality_dips.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Ranking Products by Seasonality Peaks/Dips\n",
    "# =========================\n",
    "\n",
    "# Find peak (max) and dip (min) month per product\n",
    "peaks = monthly_avg.loc[monthly_avg.groupby(\"product_id\")[\"seasonality_index\"].idxmax()].copy()\n",
    "dips  = monthly_avg.loc[monthly_avg.groupby(\"product_id\")[\"seasonality_index\"].idxmin()].copy()\n",
    "\n",
    "# Rank top 5 peaks\n",
    "top_peaks = peaks.sort_values(\"seasonality_index\", ascending=False).head(5)\n",
    "print(\"\\nTop 5 Seasonal Peaks (product_id, month, index):\")\n",
    "print(top_peaks[[\"product_id\", \"month\", \"seasonality_index\"]])\n",
    "\n",
    "# Rank top 5 dips\n",
    "top_dips = dips.sort_values(\"seasonality_index\", ascending=True).head(5)\n",
    "print(\"\\nTop 5 Seasonal Dips (product_id, month, index):\")\n",
    "print(top_dips[[\"product_id\", \"month\", \"seasonality_index\"]])\n",
    "\n",
    "# Save both tables\n",
    "peaks_out = os.path.join(seasonality_dir, \"product_seasonality_peaks.csv\")\n",
    "dips_out  = os.path.join(seasonality_dir, \"product_seasonality_dips.csv\")\n",
    "top_peaks.to_csv(peaks_out, index=False)\n",
    "top_dips.to_csv(dips_out, index=False)\n",
    "\n",
    "print(f\"Saved top peaks → {peaks_out}\")\n",
    "print(f\"Saved top dips → {dips_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28867633-b39d-457c-95d8-d7db3f4a2e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot → outputs\\seasonality\\top5_seasonal_peaks.png\n",
      "Saved plot → outputs\\seasonality\\top5_seasonal_dips.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Visualize Top Seasonal Peaks and Dips\n",
    "# =========================\n",
    "\n",
    "# --- Bar chart for Top 5 Seasonal Peaks ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(top_peaks[\"product_id\"].astype(str), top_peaks[\"seasonality_index\"], color=\"green\", edgecolor=\"black\")\n",
    "plt.title(\"Top 5 Seasonal Peaks (Highest Seasonality Index)\")\n",
    "plt.xlabel(\"Product ID\")\n",
    "plt.ylabel(\"Seasonality Index\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "peaks_plot = os.path.join(seasonality_dir, \"top5_seasonal_peaks.png\")\n",
    "plt.savefig(peaks_plot, dpi=160)\n",
    "plt.show()\n",
    "print(f\"Saved plot → {peaks_plot}\")\n",
    "\n",
    "# --- Bar chart for Top 5 Seasonal Dips ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(top_dips[\"product_id\"].astype(str), top_dips[\"seasonality_index\"], color=\"red\", edgecolor=\"black\")\n",
    "plt.title(\"Top 5 Seasonal Dips (Lowest Seasonality Index)\")\n",
    "plt.xlabel(\"Product ID\")\n",
    "plt.ylabel(\"Seasonality Index\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "dips_plot = os.path.join(seasonality_dir, \"top5_seasonal_dips.png\")\n",
    "plt.savefig(dips_plot, dpi=160)\n",
    "plt.show()\n",
    "print(f\"Saved plot → {dips_plot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26624598-2ac3-401b-abe4-c9ff2a3593a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved seasonality stability results → outputs\\seasonality\\seasonality_stability.csv\n",
      "                             product_id   end_date  stability_metric  \\\n",
      "0  1f59c631-ca0d-47d9-aac2-8da2dbc1f134 2025-09-30               1.0   \n",
      "1  1f59c631-ca0d-47d9-aac2-8da2dbc1f134 2025-10-31               1.0   \n",
      "2  1f59c631-ca0d-47d9-aac2-8da2dbc1f134 2025-11-30               1.0   \n",
      "3  1f59c631-ca0d-47d9-aac2-8da2dbc1f134 2025-12-31               1.0   \n",
      "4  1f59c631-ca0d-47d9-aac2-8da2dbc1f134 2026-01-31               1.0   \n",
      "\n",
      "        method  \n",
      "0  rolling_stl  \n",
      "1  rolling_stl  \n",
      "2  rolling_stl  \n",
      "3  rolling_stl  \n",
      "4  rolling_stl  \n",
      "Saved rolling stability plot → outputs\\seasonality\\seasonality_stability_plot.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Seasonality Stability Over Time (STL with Multi-Level Fallback)\n",
    "# =========================\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Ensure output dir\n",
    "stability_dir = os.path.join(os.path.dirname(FILE_PATH), \"outputs\", \"seasonality\")\n",
    "os.makedirs(stability_dir, exist_ok=True)\n",
    "\n",
    "def compute_seasonal_strength(series, period=12):\n",
    "    \"\"\"Compute seasonal strength for a given time series (monthly).\"\"\"\n",
    "    if len(series) < 6:  # allow shorter series\n",
    "        return None\n",
    "    try:\n",
    "        stl = STL(series, period=period, robust=True).fit()\n",
    "        var_resid = np.var(stl.resid)\n",
    "        var_season_resid = np.var(stl.seasonal + stl.resid)\n",
    "        return 1 - (var_resid / var_season_resid) if var_season_resid > 0 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Prepare monthly data (use month-end \"ME\")\n",
    "monthly_data = (\n",
    "    df.set_index(\"date\")\n",
    "      .groupby(\"product_id\")[\"price\"]\n",
    "      .resample(\"ME\").mean()\n",
    "      .reset_index(level=0)\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "results = []\n",
    "window_size = 6  # rolling window length in months\n",
    "\n",
    "# Rolling STL\n",
    "for pid, group in monthly_data.groupby(\"product_id\"):\n",
    "    group = group.dropna().set_index(\"date\").asfreq(\"ME\")\n",
    "    if len(group) < window_size:\n",
    "        continue\n",
    "    for end_idx in range(window_size, len(group) + 1):\n",
    "        window = group.iloc[end_idx-window_size:end_idx][\"price\"]\n",
    "        score = compute_seasonal_strength(window, period=12)\n",
    "        if score is not None:\n",
    "            results.append({\n",
    "                \"product_id\": pid,\n",
    "                \"end_date\": group.index[end_idx-1],\n",
    "                \"stability_metric\": score,\n",
    "                \"method\": \"rolling_stl\"\n",
    "            })\n",
    "\n",
    "stability_df = pd.DataFrame(results)\n",
    "\n",
    "# === Fallback: Full-series STL ===\n",
    "if stability_df.empty:\n",
    "    print(\"⚠️ Rolling STL gave no results — trying full-series STL.\")\n",
    "    fallback_results = []\n",
    "    for pid, group in monthly_data.groupby(\"product_id\"):\n",
    "        group = group.dropna().set_index(\"date\").asfreq(\"ME\")[\"price\"]\n",
    "        score = compute_seasonal_strength(group, period=12)\n",
    "        if score is not None:\n",
    "            fallback_results.append({\n",
    "                \"product_id\": pid,\n",
    "                \"stability_metric\": score,\n",
    "                \"method\": \"full_series_stl\"\n",
    "            })\n",
    "    stability_df = pd.DataFrame(fallback_results)\n",
    "\n",
    "# === Fallback: Rolling Std Dev ===\n",
    "if stability_df.empty:\n",
    "    print(\"⚠️ STL failed — falling back to rolling std deviation.\")\n",
    "    results = []\n",
    "    for pid, group in monthly_data.groupby(\"product_id\"):\n",
    "        group = group.dropna().set_index(\"date\").asfreq(\"ME\")\n",
    "        if len(group) < window_size:\n",
    "            continue\n",
    "        rolling_std = group[\"price\"].rolling(window=window_size).std()\n",
    "        for idx, val in rolling_std.dropna().items():\n",
    "            results.append({\n",
    "                \"product_id\": pid,\n",
    "                \"end_date\": idx,\n",
    "                \"stability_metric\": val,\n",
    "                \"method\": \"rolling_std\"\n",
    "            })\n",
    "    stability_df = pd.DataFrame(results)\n",
    "\n",
    "# Save results\n",
    "out_csv = os.path.join(stability_dir, \"seasonality_stability.csv\")\n",
    "stability_df.to_csv(out_csv, index=False)\n",
    "print(f\"Saved seasonality stability results → {out_csv}\")\n",
    "print(stability_df.head())\n",
    "\n",
    "# === Plot results ===\n",
    "if not stability_df.empty:\n",
    "    if \"end_date\" in stability_df.columns and stability_df[\"method\"].iloc[0] != \"full_series_stl\":\n",
    "        # Rolling results (STL or StdDev)\n",
    "        top_products = df[\"product_id\"].value_counts().head(3).index\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for pid in top_products:\n",
    "            subset = stability_df[stability_df[\"product_id\"] == pid]\n",
    "            if not subset.empty:\n",
    "                plt.plot(subset[\"end_date\"], subset[\"stability_metric\"], label=f\"Product {pid}\")\n",
    "        plt.title(\"Seasonality Stability Over Time (Rolling Metric)\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Stability Metric\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        stability_plot = os.path.join(stability_dir, \"seasonality_stability_plot.png\")\n",
    "        plt.savefig(stability_plot, dpi=160)\n",
    "        plt.show()\n",
    "        print(f\"Saved rolling stability plot → {stability_plot}\")\n",
    "    else:\n",
    "        # Full-series only → bar chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        top_subset = stability_df.sort_values(\"stability_metric\", ascending=False).head(10)\n",
    "        plt.bar(top_subset[\"product_id\"].astype(str), top_subset[\"stability_metric\"], color=\"skyblue\")\n",
    "        plt.title(\"Seasonality Stability (Full-Series Metric)\")\n",
    "        plt.xlabel(\"Product ID\")\n",
    "        plt.ylabel(\"Stability Metric\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        stability_plot = os.path.join(stability_dir, \"seasonality_stability_bar.png\")\n",
    "        plt.savefig(stability_plot, dpi=160)\n",
    "        plt.show()\n",
    "        print(f\"Saved full-series stability plot → {stability_plot}\")\n",
    "else:\n",
    "    print(\" No stability results at all — dataset may be too small.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b88e082-ffbe-4023-8154-d0658bbb7b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: ['_id', 'product_id', 'date', 'price', 'day', 'month']\n",
      " No 'promo' or 'event' column in dataset — skipping analysis.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Promotion / Event Impact Analysis\n",
    "# =========================\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "\n",
    "# Ensure output dir\n",
    "event_dir = os.path.join(os.path.dirname(FILE_PATH), \"outputs\", \"seasonality\")\n",
    "os.makedirs(event_dir, exist_ok=True)\n",
    "\n",
    "# Check for promo/event columns\n",
    "available_cols = df.columns.tolist()\n",
    "print(\"Available columns:\", available_cols)\n",
    "\n",
    "if \"promo\" in df.columns or \"event\" in df.columns:\n",
    "    df_event = df.copy()\n",
    "    df_event[\"month\"] = df_event[\"date\"].dt.month\n",
    "\n",
    "    results_summary = {}\n",
    "\n",
    "    # --- ANOVA tests ---\n",
    "    if \"promo\" in df_event.columns:\n",
    "        grouped = [group[\"price\"].dropna().values for _, group in df_event.groupby(\"promo\")]\n",
    "        if len(grouped) > 1:\n",
    "            f_val, p_val = stats.f_oneway(*grouped)\n",
    "            results_summary[\"promo_anova_f\"] = f_val\n",
    "            results_summary[\"promo_anova_p\"] = p_val\n",
    "\n",
    "    if \"event\" in df_event.columns:\n",
    "        grouped = [group[\"price\"].dropna().values for _, group in df_event.groupby(\"event\")]\n",
    "        if len(grouped) > 1:\n",
    "            f_val, p_val = stats.f_oneway(*grouped)\n",
    "            results_summary[\"event_anova_f\"] = f_val\n",
    "            results_summary[\"event_anova_p\"] = p_val\n",
    "\n",
    "    # --- Regression model (fixed effects via C()) ---\n",
    "    formula = \"price ~ \"\n",
    "    predictors = []\n",
    "    if \"promo\" in df_event.columns:\n",
    "        predictors.append(\"promo\")\n",
    "    if \"event\" in df_event.columns:\n",
    "        predictors.append(\"event\")\n",
    "\n",
    "    formula += \" + \".join(predictors) + \" + C(month) + C(product_id)\"\n",
    "\n",
    "    model = smf.ols(formula=formula, data=df_event).fit()\n",
    "    print(model.summary())\n",
    "\n",
    "    # Save regression results\n",
    "    coef_df = model.summary2().tables[1].reset_index()\n",
    "    coef_df.to_csv(os.path.join(event_dir, \"event_effects.csv\"), index=False)\n",
    "    print(f\"Saved regression effects → {os.path.join(event_dir, 'event_effects.csv')}\")\n",
    "\n",
    "    # --- Plot average prices by promo/event ---\n",
    "    if \"promo\" in df_event.columns:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        df_event.groupby(\"promo\")[\"price\"].mean().plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n",
    "        plt.title(\"Average Price by Promo\")\n",
    "        plt.ylabel(\"Mean Price ($)\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        promo_plot = os.path.join(event_dir, \"avg_price_by_promo.png\")\n",
    "        plt.savefig(promo_plot, dpi=160)\n",
    "        plt.show()\n",
    "        print(f\"Saved plot → {promo_plot}\")\n",
    "\n",
    "    if \"event\" in df_event.columns:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        df_event.groupby(\"event\")[\"price\"].mean().plot(kind=\"bar\", color=\"coral\", edgecolor=\"black\")\n",
    "        plt.title(\"Average Price by Event\")\n",
    "        plt.ylabel(\"Mean Price ($)\")\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        event_plot = os.path.join(event_dir, \"avg_price_by_event.png\")\n",
    "        plt.savefig(event_plot, dpi=160)\n",
    "        plt.show()\n",
    "        print(f\"Saved plot → {event_plot}\")\n",
    "\n",
    "else:\n",
    "    print(\" No 'promo' or 'event' column in dataset — skipping analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81d6c33f-f63c-4199-8bb3-2ec5b08064cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved forecast metrics → outputs\\seasonality\\forecast_metrics.csv\n",
      "                             product_id  naive_smape  naive_mase  arima_smape  \\\n",
      "0  1f59c631-ca0d-47d9-aac2-8da2dbc1f134     5.892647    2.291740     6.445604   \n",
      "1  ba6585d8-4881-4569-a4ef-e458cb9dabd6     5.235371    2.349287     6.308204   \n",
      "2  f596dfea-1a5b-4173-aa60-1fbad5af36da     5.507919    2.668875     7.081364   \n",
      "\n",
      "   arima_mase  \n",
      "0    2.447247  \n",
      "1    2.447679  \n",
      "2    3.076456  \n",
      "Saved leaderboard plot → outputs\\seasonality\\forecast_leaderboard.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Rolling Backtest & Leaderboard (Looser Conditions)\n",
    "# =========================\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "results = []\n",
    "\n",
    "top_products = df[\"product_id\"].value_counts().head(3).index\n",
    "\n",
    "for pid in top_products:\n",
    "    prod_series = monthly_data[monthly_data[\"product_id\"] == pid].dropna()\n",
    "    prod_series = prod_series.set_index(\"date\")[\"price\"]\n",
    "\n",
    "    if len(prod_series) < 6:  # require at least 6 months\n",
    "        continue\n",
    "\n",
    "    # --- One-step-ahead rolling forecast ---\n",
    "    smape_naive, mase_naive = [], []\n",
    "    smape_arima, mase_arima = [], []\n",
    "\n",
    "    for t in range(3, len(prod_series)):  # start after 3 points\n",
    "        train = prod_series.iloc[:t]\n",
    "        test = prod_series.iloc[t:t+1]\n",
    "\n",
    "        # --- Seasonal Naive ---\n",
    "        if len(train) >= 12:\n",
    "            y_pred_naive = [train.iloc[-12]]  # same month last year\n",
    "        else:\n",
    "            y_pred_naive = [train.iloc[-1]]  # fallback to last value\n",
    "\n",
    "        smape_naive.append(smape(test.values, y_pred_naive))\n",
    "        mase_naive.append(mase(test.values, y_pred_naive, train))\n",
    "\n",
    "        # --- ARIMA ---\n",
    "        try:\n",
    "            model = ARIMA(train, order=(1,1,1))\n",
    "            fitted = model.fit()\n",
    "            forecast = fitted.forecast(steps=1)\n",
    "            smape_arima.append(smape(test.values, forecast.values))\n",
    "            mase_arima.append(mase(test.values, forecast.values, train))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    results.append({\n",
    "        \"product_id\": pid,\n",
    "        \"naive_smape\": np.mean(smape_naive) if smape_naive else None,\n",
    "        \"naive_mase\": np.mean(mase_naive) if mase_naive else None,\n",
    "        \"arima_smape\": np.mean(smape_arima) if smape_arima else None,\n",
    "        \"arima_mase\": np.mean(mase_arima) if mase_arima else None\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "leaderboard = pd.DataFrame(results)\n",
    "out_csv = os.path.join(stability_dir, \"forecast_metrics.csv\")\n",
    "leaderboard.to_csv(out_csv, index=False)\n",
    "print(f\"Saved forecast metrics → {out_csv}\")\n",
    "print(leaderboard)\n",
    "\n",
    "# Leaderboard plot\n",
    "if not leaderboard.empty:\n",
    "    avg_scores = leaderboard.drop(columns=[\"product_id\"]).mean()\n",
    "    plt.figure(figsize=(8,5))\n",
    "    avg_scores.plot(kind=\"bar\", color=[\"skyblue\",\"skyblue\",\"coral\",\"coral\"], edgecolor=\"black\")\n",
    "    plt.title(\"Forecast Leaderboard (Lower = Better)\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    leaderboard_plot = os.path.join(stability_dir, \"forecast_leaderboard.png\")\n",
    "    plt.savefig(leaderboard_plot, dpi=160)\n",
    "    plt.show()\n",
    "    print(f\"Saved leaderboard plot → {leaderboard_plot}\")\n",
    "else:\n",
    "    print(\"No forecast results available. Check data coverage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc950b52-efd6-4281-b6c7-85c98de46e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic 'category' column created from product_id prefix.\n",
      "Saved category seasonality index → outputs\\category_seasonality\\category_seasonality_index.csv\n",
      "Saved category heatmap → outputs\\category_seasonality\\category_seasonality_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import warnings\n",
    "FILE_PATH = r\"AugmentedData.product_pricing_full_year_5day.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df = df.sort_values(\"date\")\n",
    "\n",
    "# Ensure required columns exist\n",
    "expected_cols = {\"date\", \"price\", \"product_id\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "# Create synthetic category (first 3 characters of product_id)\n",
    "df[\"category\"] = df[\"product_id\"].astype(str).str[:3]\n",
    "print(\"Synthetic 'category' column created from product_id prefix.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "#  Category-Level Seasonality Indices\n",
    "# =========================\n",
    "cat_seasonality_dir = os.path.join(os.path.dirname(FILE_PATH), \"outputs\", \"category_seasonality\")\n",
    "os.makedirs(cat_seasonality_dir, exist_ok=True)\n",
    "\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "\n",
    "cat_monthly_avg = (\n",
    "    df.groupby([\"category\", \"month\"])[\"price\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "cat_long_run = (\n",
    "    df.groupby(\"category\")[\"price\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"price\": \"long_run_mean\"})\n",
    ")\n",
    "\n",
    "cat_monthly_avg = cat_monthly_avg.merge(cat_long_run, on=\"category\")\n",
    "cat_monthly_avg[\"seasonality_index\"] = cat_monthly_avg[\"price\"] / cat_monthly_avg[\"long_run_mean\"]\n",
    "\n",
    "# Save CSV\n",
    "cat_csv = os.path.join(cat_seasonality_dir, \"category_seasonality_index.csv\")\n",
    "cat_monthly_avg.to_csv(cat_csv, index=False)\n",
    "print(f\"Saved category seasonality index → {cat_csv}\")\n",
    "\n",
    "# Save Heatmap\n",
    "cat_pivot = cat_monthly_avg.pivot(index=\"category\", columns=\"month\", values=\"seasonality_index\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(cat_pivot, cmap=\"coolwarm\", center=1, annot=True, fmt=\".2f\", cbar_kws={'label': 'Seasonality Index'})\n",
    "plt.title(\"Category-Level Seasonality Indices (relative to long-run mean)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "\n",
    "cat_heatmap = os.path.join(cat_seasonality_dir, \"category_seasonality_heatmap.png\")\n",
    "plt.savefig(cat_heatmap, dpi=160)\n",
    "plt.close()\n",
    "print(f\"Saved category heatmap → {cat_heatmap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa4e86d2-ebbd-4160-a87d-7c3d1e45d685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved category forecast leaderboard → outputs\\category_forecast\\category_forecast_leaderboard.csv\n",
      "Saved forecast plot → outputs\\category_forecast\\category_forecast_leaderboard.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Category-Level Forecast Leaderboard\n",
    "# =========================\n",
    "warnings.filterwarnings(\"ignore\", message=\"Too few observations to estimate starting parameters\")\n",
    "\n",
    "forecast_dir = os.path.join(os.path.dirname(FILE_PATH), \"outputs\", \"category_forecast\")\n",
    "os.makedirs(forecast_dir, exist_ok=True)\n",
    "\n",
    "forecast_results = []\n",
    "\n",
    "for category, group in df.groupby(\"category\"):\n",
    "    group = group.sort_values(\"date\").set_index(\"date\")\n",
    "    weekly_series = group[\"price\"].resample(\"W\").mean().dropna()\n",
    "    \n",
    "    if len(weekly_series) < 30:\n",
    "        continue\n",
    "    \n",
    "    train = weekly_series.iloc[:-12]\n",
    "    test = weekly_series.iloc[-12:]\n",
    "    \n",
    "    # Seasonal Naive\n",
    "    naive_forecast = pd.Series(train.iloc[-12:].values, index=test.index)\n",
    "    \n",
    "    # ARIMA\n",
    "    try:\n",
    "        arima = ARIMA(train, order=(1,1,1), seasonal_order=(0,1,1,52))\n",
    "        arima_fit = arima.fit()\n",
    "        arima_forecast = arima_fit.forecast(steps=12)\n",
    "    except:\n",
    "        arima_forecast = pd.Series([np.nan]*12, index=test.index)\n",
    "    \n",
    "    # ETS\n",
    "    try:\n",
    "        ets = ExponentialSmoothing(train, seasonal=\"add\", seasonal_periods=52)\n",
    "        ets_fit = ets.fit()\n",
    "        ets_forecast = ets_fit.forecast(12)\n",
    "    except:\n",
    "        ets_forecast = pd.Series([np.nan]*12, index=test.index)\n",
    "    \n",
    "    # Metrics\n",
    "    def smape(y_true, y_pred):\n",
    "        return 100/len(y_true) * np.sum(\n",
    "            2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred))\n",
    "        )\n",
    "    \n",
    "    def mase(y_true, y_pred, y_train):\n",
    "        naive = np.mean(np.abs(np.diff(y_train)))\n",
    "        return np.mean(np.abs(y_true - y_pred)) / naive if naive != 0 else np.nan\n",
    "    \n",
    "    for model_name, preds in [(\"Naive\", naive_forecast),\n",
    "                              (\"ARIMA\", arima_forecast),\n",
    "                              (\"ETS\", ets_forecast)]:\n",
    "        forecast_results.append({\n",
    "            \"category\": category,\n",
    "            \"model\": model_name,\n",
    "            \"sMAPE\": smape(test.values, preds.values),\n",
    "            \"MASE\": mase(test.values, preds.values, train.values)\n",
    "        })\n",
    "\n",
    "forecast_df = pd.DataFrame(forecast_results)\n",
    "forecast_csv = os.path.join(forecast_dir, \"category_forecast_leaderboard.csv\")\n",
    "forecast_df.to_csv(forecast_csv, index=False)\n",
    "print(f\"Saved category forecast leaderboard → {forecast_csv}\")\n",
    "\n",
    "# Save leaderboard plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=forecast_df, x=\"model\", y=\"sMAPE\")\n",
    "plt.title(\"Category-Level Forecasting Performance (sMAPE Distribution)\")\n",
    "plt.ylabel(\"sMAPE (%)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.tight_layout()\n",
    "\n",
    "forecast_plot = os.path.join(forecast_dir, \"category_forecast_leaderboard.png\")\n",
    "plt.savefig(forecast_plot, dpi=160)\n",
    "plt.close()\n",
    "print(f\"Saved forecast plot → {forecast_plot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96781286-7cdd-4b57-9079-1ec0fb630287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved correlation heatmap → outputs\\correlation\\category_correlation_heatmap.png\n",
      "Saved correlation matrix → outputs\\correlation\\category_correlation_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cross-Category Correlation Heatmap\n",
    "# =========================\n",
    "corr_dir = os.path.join(os.path.dirname(FILE_PATH), \"outputs\", \"correlation\")\n",
    "os.makedirs(corr_dir, exist_ok=True)\n",
    "\n",
    "category_prices = (\n",
    "    df.groupby([\"date\", \"category\"])[\"price\"]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    "      .pivot(index=\"date\", columns=\"category\", values=\"price\")\n",
    ")\n",
    "\n",
    "corr_matrix = category_prices.corr()\n",
    "\n",
    "# Save correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\n",
    "plt.title(\"Cross-Category Price Correlation Heatmap\")\n",
    "plt.tight_layout()\n",
    "\n",
    "corr_plot = os.path.join(corr_dir, \"category_correlation_heatmap.png\")\n",
    "plt.savefig(corr_plot, dpi=160)\n",
    "plt.close()\n",
    "print(f\"Saved correlation heatmap → {corr_plot}\")\n",
    "\n",
    "# Save correlation matrix CSV\n",
    "corr_csv = os.path.join(corr_dir, \"category_correlation_matrix.csv\")\n",
    "corr_matrix.to_csv(corr_csv)\n",
    "print(f\"Saved correlation matrix → {corr_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
