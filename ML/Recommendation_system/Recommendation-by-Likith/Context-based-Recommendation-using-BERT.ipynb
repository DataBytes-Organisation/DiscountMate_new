{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setuptools.dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('Coles_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load DistilBERT tokenizer and model\n",
    "# Specify the pre-trained model weights for DistilBERT\n",
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "\n",
    "# Load the DistilBERT tokenizer using the pre-trained weights\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "# Load the DistilBERT model using the pre-trained weights\n",
    "bert_model = TFDistilBertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract product names for tokenization\\n\",\n",
    "product_names = df['item_name'].fillna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32  # Number of product names per batch\n",
    "embedding_save_path = \"product_embeddings\"  # Directory to save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (20608, 768)\n",
      "Embeddings saved in folder: product_embeddings\n"
     ]
    }
   ],
   "source": [
    "product_embeddings = []\n",
    "for i in range(0, len(product_names), batch_size):\n",
    "    # Get the current batch\n",
    "    batch = product_names[i:i + batch_size]\n",
    "    \n",
    "    # Tokenize the batch\n",
    "    tokenized_batch = tokenizer(batch, padding=True, truncation=True, max_length=20, return_tensors=\"tf\")\n",
    "    \n",
    "    # Generate embeddings using DistilBERT\n",
    "    output = bert_model(tokenized_batch['input_ids'])\n",
    "    mean_pooled = tf.reduce_mean(output.last_hidden_state, axis=1).numpy()  # Mean pooling\n",
    "\n",
    "    # Append to the embeddings list\n",
    "    product_embeddings.extend(mean_pooled)\n",
    "\n",
    "    # Save each batch inside the folder\n",
    "    batch_file_path = os.path.join(embedding_save_path, f\"batch_{i // batch_size}.npy\")\n",
    "    np.save(batch_file_path, mean_pooled)\n",
    "\n",
    "# Convert all embeddings to a NumPy array\n",
    "product_embeddings = np.array(product_embeddings)\n",
    "\n",
    "# Save all embeddings to a single file inside the folder\n",
    "all_embeddings_file_path = os.path.join(embedding_save_path, \"all_embeddings.npy\")\n",
    "np.save(all_embeddings_file_path, product_embeddings)\n",
    "\n",
    "# Load saved embeddings (for reuse)\n",
    "loaded_embeddings = np.load(all_embeddings_file_path)\n",
    "\n",
    "print(\"Embeddings shape:\", loaded_embeddings.shape)\n",
    "print(f\"Embeddings saved in folder: {embedding_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_embeddings = np.load(\"product_embeddings/all_embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(product_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a recommendation function\n",
    "def recommend_products(product_name, top_n=5):\n",
    "    # Find the index of the input product\n",
    "    if product_name not in product_names:\n",
    "        return f\"Product '{product_name}' not found in the dataset.\"\n",
    "    \n",
    "    idx = product_names.index(product_name)\n",
    "    # Get indices of top N similar products (excluding the input product itself)\n",
    "    similar_indices = np.argsort(similarity_matrix[idx])[::-1][1:top_n + 1]\n",
    "    # Fetch product names based on indices\n",
    "    recommendations = [product_names[i] for i in similar_indices]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for 'Coles Boneless Pork Leg Roast | approx 2.1kg': ['Coles Boneless Pork Shoulder Roast | approx 2.6kg', 'Coles Pork Belly Roast Boneless | approx 1.3kg', 'Coles Lamb Boneless Shoulder Roast | approx 1.3kg', 'Coles Butcher Lamb Leg Roast Boneless | approx 1.08kg', 'Coles Lamb Whole Lamb Leg Roast | approx 2.8kg']\n"
     ]
    }
   ],
   "source": [
    "# Example recommendation for the first product\n",
    "example_product = product_names[0]\n",
    "recommendations = recommend_products(example_product, top_n=5)\n",
    "print(f\"Recommendations for '{example_product}': {recommendations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
