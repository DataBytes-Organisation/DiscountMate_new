{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe746c31",
   "metadata": {},
   "source": [
    "# Deal Prediction Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d1dd2",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "438618cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install xgboost\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from pathlib import Path\n",
    "import sys, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab9bec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "output = Path(r\"C:/Users/pmayr/Downloads/Output\")\n",
    "data_path = output/\"staged_features_events_brands_size.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77cb04d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmayr\\AppData\\Local\\Temp\\ipykernel_16748\\1617873353.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "if \"scrape_date\" not in df.columns:\n",
    "    df[\"scrape_date\"] = pd.to_datetime(df[\"scrape_date_str\"], errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8dc9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47503, 36)        sku scrape_date category\n",
      "0  8371390  2025-04-02   Easter\n",
      "1  7473849  2025-04-02   Easter\n",
      "2  5726070  2025-04-02   Easter\n"
     ]
    }
   ],
   "source": [
    "#Keeping only yhe rows withe the sku and  date\n",
    "df = df.dropna(subset=[\"sku\", \"scrape_date\"]).copy()\n",
    "\n",
    "# fill discounts safely\n",
    "if \"discount_percentage\" in df.columns:\n",
    "    df[\"discount_percentage\"] = df[\"discount_percentage\"].fillna(0.0)\n",
    "if \"discount_pct_filled\" in df.columns:\n",
    "    df[\"discount_pct_filled\"] = df[\"discount_pct_filled\"].fillna(0.0)\n",
    "\n",
    "# cast some categoricals (optional)\n",
    "for col in [\"brand_tier\", \"size_band\", \"season\", \"category\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "print(df.shape, df[[\"sku\",\"scrape_date\",\"category\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "244e105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels] non-null rates: days 0.49889480664379093 | pct 0.49889480664379093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmayr\\AppData\\Local\\Temp\\ipykernel_16748\\382189985.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  panel = df.groupby(\"sku\", group_keys=False).apply(build_forward_labels_with_fallback)\n"
     ]
    }
   ],
   "source": [
    "# ===== LABELS with fallback to next observation =====\n",
    "def build_forward_labels_with_fallback(sub: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = sub.sort_values(\"scrape_date\").reset_index(drop=True)\n",
    "    n = len(sub)\n",
    "    next_days = np.full(n, np.nan, dtype=float)\n",
    "    next_pct  = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    # Define promo condition robustly\n",
    "    has_disc_col = \"discount_pct_filled\" in sub.columns\n",
    "    on_promo = (\n",
    "        (sub[\"is_on_promo\"] == 1)\n",
    "        | (sub[\"discount_percentage\"].fillna(0) > 0 if \"discount_percentage\" in sub.columns else False)\n",
    "        | (sub[\"discount_pct_filled\"].fillna(0) > 0 if has_disc_col else False)\n",
    "    ).to_numpy()\n",
    "\n",
    "    promo_idx = np.where(on_promo)[0]\n",
    "\n",
    "    for i in range(n - 1):  # last row can’t have a future label\n",
    "        # 1) try next promo strictly after i\n",
    "        j_candidates = promo_idx[promo_idx > i]\n",
    "        if len(j_candidates) > 0:\n",
    "            j = j_candidates[0]\n",
    "        else:\n",
    "            # 2) fallback to the very next observation\n",
    "            j = i + 1\n",
    "\n",
    "        # days until j\n",
    "        next_days[i] = (sub.loc[j, \"scrape_date\"] - sub.loc[i, \"scrape_date\"]).days\n",
    "\n",
    "        # discount % at j (prefer filled)\n",
    "        if has_disc_col and pd.notna(sub.loc[j, \"discount_pct_filled\"]):\n",
    "            next_pct[i] = float(sub.loc[j, \"discount_pct_filled\"])\n",
    "        elif \"discount_percentage\" in sub.columns and pd.notna(sub.loc[j, \"discount_percentage\"]):\n",
    "            next_pct[i] = float(sub.loc[j, \"discount_percentage\"])\n",
    "        else:\n",
    "            next_pct[i] = 0.0  # safe fallback\n",
    "\n",
    "    sub[\"y_days_to_next_discount\"] = next_days\n",
    "    sub[\"y_next_discount_pct\"]     = next_pct\n",
    "    return sub\n",
    "\n",
    "# rebuild panel with the new labels\n",
    "panel = df.groupby(\"sku\", group_keys=False).apply(build_forward_labels_with_fallback)\n",
    "print(\"[labels] non-null rates:\",\n",
    "      \"days\", panel[\"y_days_to_next_discount\"].notna().mean(),\n",
    "      \"| pct\", panel[\"y_next_discount_pct\"].notna().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1fe494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X leak-free] shape: (47503, 39) | n_features: 39\n",
      "  sample features: ['item_price_lag1', 'b_price_lag1', 'b_price_lag2', 'b_price_roll3_mean', 'b_price_roll3_std', 'b_price_roll7_mean', 'item_price_lag2', 'item_price_roll3_mean', 'item_price_roll3_std', 'item_price_roll7_mean']\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) FEATURES: NUMERIC + LAGS + COMPACT CAT CODES (no one-hot) =====\n",
    "# numeric features to use\n",
    "num_cols = [c for c in [\n",
    "    \"b_price\",\"item_price\",\"original_price\",\"b_unit_price\",\"item_unit_price\",\n",
    "    \"price_gap\",\"unit_price_gap\",\n",
    "    \"discount_pct_filled\"\n",
    "] if c in panel.columns]\n",
    "\n",
    "# create lags\n",
    "panel = panel.sort_values([\"sku\",\"scrape_date\"]).copy()\n",
    "\n",
    "def add_lags_and_rolls(df, col, group=\"sku\"):\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    g = df.groupby(group)[col]\n",
    "    df[f\"{col}_lag1\"] = g.shift(1)\n",
    "    df[f\"{col}_lag2\"] = g.shift(2)\n",
    "    # short-term trend & stability\n",
    "    df[f\"{col}_roll3_mean\"] = g.shift(1).rolling(3, min_periods=1).mean()\n",
    "    df[f\"{col}_roll3_std\"]  = g.shift(1).rolling(3, min_periods=1).std()\n",
    "    # longer horizon signal\n",
    "    df[f\"{col}_roll7_mean\"] = g.shift(1).rolling(7, min_periods=1).mean()\n",
    "\n",
    "# numeric cols we can learn from (NO same-day discount fields)\n",
    "num_safe = [c for c in [\n",
    "    \"b_price\",\"item_price\",\"original_price\",\n",
    "    \"b_unit_price\",\"item_unit_price\",\n",
    "    \"price_gap\",\"unit_price_gap\"\n",
    "] if c in panel.columns]\n",
    "\n",
    "for c in num_safe:\n",
    "    add_lags_and_rolls(panel, c)\n",
    "\n",
    "# compact categoricals as codes (NO same-day text flags)\n",
    "def add_compact_code(df, col, top_k=30):\n",
    "    if col not in df.columns: return None\n",
    "    s = df[col].astype(str)\n",
    "    top = set(s.value_counts(dropna=False).head(top_k).index)\n",
    "    safe = s.where(s.isin(top), \"OTHER\").astype(\"category\")\n",
    "    code_col = f\"{col}_code\"\n",
    "    df[code_col] = safe.cat.codes.astype(\"int16\")\n",
    "    return code_col\n",
    "\n",
    "code_cols = []\n",
    "for c in [\"brand_tier\",\"size_band\",\"season\",\"category\"]:\n",
    "    cc = add_compact_code(panel, c, top_k=30)\n",
    "    if cc: code_cols.append(cc)\n",
    "\n",
    "# ==== FINAL, LEAK-FREE FEATURE LIST ====\n",
    "# Only lags/rolls + compact codes. NO current discount fields.\n",
    "lagroll_cols = [c for c in panel.columns if any(\n",
    "    c.startswith(k+\"_lag\") or c.startswith(k+\"_roll\") for k in num_safe\n",
    ")]\n",
    "X_cols = lagroll_cols + code_cols\n",
    "\n",
    "# Build X, y\n",
    "X = panel[X_cols].astype(\"float32\")\n",
    "y_days = panel[\"y_days_to_next_discount\"]\n",
    "y_disc = panel[\"y_next_discount_pct\"]\n",
    "\n",
    "print(\"[X leak-free] shape:\", X.shape, \"| n_features:\", len(X_cols))\n",
    "print(\"  sample features:\", X_cols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35ec39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days mask counts: 23011 95 593\n",
      "Pct  mask counts: 23011 95 593\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) TEMPORAL SPLIT (WIDENED) =====\n",
    "dates_sorted = panel[\"scrape_date\"].dropna().unique()\n",
    "dates_sorted = np.sort(dates_sorted)\n",
    "n_dates = len(dates_sorted)\n",
    "\n",
    "if n_dates >= 4:\n",
    "    # last = TEST, second last = VAL, rest = TRAIN\n",
    "    test_dates = set(dates_sorted[-1:])\n",
    "    val_dates  = set(dates_sorted[-2:-1])\n",
    "    mask_test = panel[\"scrape_date\"].isin(test_dates)\n",
    "    mask_val  = panel[\"scrape_date\"].isin(val_dates)\n",
    "    mask_train = ~(mask_test | mask_val)\n",
    "\n",
    "elif n_dates == 3:\n",
    "    # last = TEST, second last = VAL, first = TRAIN\n",
    "    test_dates = set(dates_sorted[-1:])\n",
    "    val_dates  = set(dates_sorted[-2:-1])\n",
    "    mask_test = panel[\"scrape_date\"].isin(test_dates)\n",
    "    mask_val  = panel[\"scrape_date\"].isin(val_dates)\n",
    "    mask_train = ~(mask_test | mask_val)\n",
    "\n",
    "else:\n",
    "    # 1–2 scrape dates → fallback: 80/20 split\n",
    "    order = panel[\"scrape_date\"].rank(method=\"first\")\n",
    "    q80 = order.quantile(0.8)\n",
    "    mask_train = order <= q80\n",
    "    mask_val   = (order > q80)\n",
    "    mask_test  = (order > q80)\n",
    "\n",
    "# apply to targets\n",
    "m_days = y_days.notna()\n",
    "m_disc = y_disc.notna()\n",
    "\n",
    "mask_tr_days  = (mask_train & m_days).to_numpy()\n",
    "mask_va_days  = (mask_val   & m_days).to_numpy()\n",
    "mask_te_days  = (mask_test  & m_days).to_numpy()\n",
    "\n",
    "mask_tr_disc  = (mask_train & m_disc).to_numpy()\n",
    "mask_va_disc  = (mask_val   & m_disc).to_numpy()\n",
    "mask_te_disc  = (mask_test  & m_disc).to_numpy()\n",
    "\n",
    "print(\"Days mask counts:\", mask_tr_days.sum(), mask_va_days.sum(), mask_te_days.sum())\n",
    "print(\"Pct  mask counts:\", mask_tr_disc.sum(), mask_va_disc.sum(), mask_te_disc.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "702102a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days (clean): (23011, 39) (95, 39) (593, 39)\n",
      "Pct  (clean): (23011, 39) (95, 39) (593, 39)\n"
     ]
    }
   ],
   "source": [
    "def sanitize_with_mask(X_df: pd.DataFrame, y_ser: pd.Series, mask: np.ndarray):\n",
    "    Xc = X_df[mask]\n",
    "    yc = y_ser[mask]\n",
    "    yc = yc.replace([np.inf, -np.inf], np.nan)\n",
    "    m_y = np.isfinite(yc.to_numpy())\n",
    "    Xc = Xc.iloc[m_y]\n",
    "    yc = yc.iloc[m_y]\n",
    "    Xc = Xc.replace([np.inf, -np.inf], np.nan).astype(np.float32, copy=False)\n",
    "    yc = yc.astype(np.float32, copy=False)\n",
    "    return Xc, yc\n",
    "\n",
    "\n",
    "# Build clean matrices using masks (NO .loc with giant indexers)\n",
    "Xd_tr, yd_tr = sanitize_with_mask(X, y_days, mask_tr_days)\n",
    "Xd_va, yd_va = sanitize_with_mask(X, y_days, mask_va_days)\n",
    "Xd_te, yd_te = sanitize_with_mask(X, y_days, mask_te_days)\n",
    "\n",
    "Xr_tr, yr_tr = sanitize_with_mask(X, y_disc, mask_tr_disc)\n",
    "Xr_va, yr_va = sanitize_with_mask(X, y_disc, mask_va_disc)\n",
    "Xr_te, yr_te = sanitize_with_mask(X, y_disc, mask_te_disc)\n",
    "\n",
    "print(\"Days (clean):\", Xd_tr.shape, Xd_va.shape, Xd_te.shape)\n",
    "print(\"Pct  (clean):\", Xr_tr.shape, Xr_va.shape, Xr_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "803a32da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[dates] unique: 9\n",
      "scrape_date\n",
      "2025-04-02      256\n",
      "2025-04-03      134\n",
      "2025-04-08      156\n",
      "2025-04-09      336\n",
      "2025-04-10       68\n",
      "2025-04-17    44832\n",
      "2025-05-02      340\n",
      "2025-05-08      191\n",
      "2025-05-15     1190\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[labels non-null]\n",
      "  y_days_to_next_discount: 23699\n",
      "  y_next_discount_pct    : 23699\n",
      "\n",
      "[mask counts BEFORE sanitize]\n",
      "  days  train/val/test: 23011 95 593\n",
      "  pct   train/val/test: 23011 95 593\n",
      "\n",
      "[X null rates (top 10)]\n",
      "pct_chg_item_price    0.861272\n",
      "item_price_lag1       0.501105\n",
      "b_unit_price          0.046229\n",
      "item_unit_price       0.046229\n",
      "unit_price_gap        0.046229\n",
      "b_price               0.000000\n",
      "item_price            0.000000\n",
      "original_price        0.000000\n",
      "price_gap             0.000000\n",
      "brand_tier_code       0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ==== DIAGNOSTICS ====\n",
    "print(\"\\n[dates] unique:\", panel[\"scrape_date\"].nunique())\n",
    "print(panel[\"scrape_date\"].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n[labels non-null]\")\n",
    "print(\"  y_days_to_next_discount:\", panel[\"y_days_to_next_discount\"].notna().sum())\n",
    "print(\"  y_next_discount_pct    :\", panel[\"y_next_discount_pct\"].notna().sum())\n",
    "\n",
    "print(\"\\n[mask counts BEFORE sanitize]\")\n",
    "print(\"  days  train/val/test:\",\n",
    "      (mask_train & panel[\"y_days_to_next_discount\"].notna()).sum(),\n",
    "      (mask_val   & panel[\"y_days_to_next_discount\"].notna()).sum(),\n",
    "      (mask_test  & panel[\"y_days_to_next_discount\"].notna()).sum())\n",
    "\n",
    "print(\"  pct   train/val/test:\",\n",
    "      (mask_train & panel[\"y_next_discount_pct\"].notna()).sum(),\n",
    "      (mask_val   & panel[\"y_next_discount_pct\"].notna()).sum(),\n",
    "      (mask_test  & panel[\"y_next_discount_pct\"].notna()).sum())\n",
    "\n",
    "print(\"\\n[X null rates (top 10)]\")\n",
    "print(X.isna().mean().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df005802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels rebuilt] days non-null: 23699 | pct non-null: 23699\n"
     ]
    }
   ],
   "source": [
    "def build_forward_labels_with_fallback(sub: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = sub.sort_values(\"scrape_date\").reset_index(drop=True)\n",
    "    n = len(sub)\n",
    "    next_days = np.full(n, np.nan, dtype=float)\n",
    "    next_pct  = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    # robust promo flag\n",
    "    on_promo = (\n",
    "        (sub.get(\"is_on_promo\", 0) == 1) |\n",
    "        (sub.get(\"discount_percentage\", 0).fillna(0) > 0) |\n",
    "        (sub.get(\"discount_pct_filled\", 0).fillna(0) > 0)\n",
    "    ).to_numpy()\n",
    "\n",
    "    promo_idx = np.where(on_promo)[0]\n",
    "    for i in range(n - 1):\n",
    "        j_candidates = promo_idx[promo_idx > i]\n",
    "        j = j_candidates[0] if len(j_candidates) else i + 1\n",
    "\n",
    "        next_days[i] = (sub.loc[j, \"scrape_date\"] - sub.loc[i, \"scrape_date\"]).days\n",
    "\n",
    "        v = None\n",
    "        if \"discount_pct_filled\" in sub.columns and pd.notna(sub.loc[j, \"discount_pct_filled\"]):\n",
    "            v = float(sub.loc[j, \"discount_pct_filled\"])\n",
    "        elif \"discount_percentage\" in sub.columns and pd.notna(sub.loc[j, \"discount_percentage\"]):\n",
    "            v = float(sub.loc[j, \"discount_percentage\"])\n",
    "        next_pct[i] = 0.0 if v is None else v\n",
    "\n",
    "    sub[\"y_days_to_next_discount\"] = next_days\n",
    "    sub[\"y_next_discount_pct\"]     = next_pct\n",
    "    return sub\n",
    "\n",
    "# Rebuild panel (ensure scrape_date is datetime)\n",
    "if \"scrape_date\" not in df.columns:\n",
    "    df[\"scrape_date\"] = pd.to_datetime(df.get(\"scrape_date_str\"), errors=\"coerce\")\n",
    "    panel = df.groupby(\"sku\", group_keys=False).apply(build_forward_labels_with_fallback)\n",
    "print(\"[labels rebuilt] days non-null:\", panel[\"y_days_to_next_discount\"].notna().sum(),\n",
    "      \"| pct non-null:\", panel[\"y_next_discount_pct\"].notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23ac3372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days mask counts: 23011 95 593\n",
      "Pct  mask counts: 23011 95 593\n"
     ]
    }
   ],
   "source": [
    "# ===== TEMPORAL SPLIT (WIDENED) =====\n",
    "dates_sorted = np.sort(panel[\"scrape_date\"].dropna().unique())\n",
    "n_dates = len(dates_sorted)\n",
    "\n",
    "if n_dates >= 4:\n",
    "    test_dates = set(dates_sorted[-1:])\n",
    "    val_dates  = set(dates_sorted[-2:-1])\n",
    "    mask_test  = panel[\"scrape_date\"].isin(test_dates)\n",
    "    mask_val   = panel[\"scrape_date\"].isin(val_dates)\n",
    "    mask_train = ~(mask_test | mask_val)\n",
    "elif n_dates == 3:\n",
    "    test_dates = set(dates_sorted[-1:])\n",
    "    val_dates  = set(dates_sorted[-2:-1])\n",
    "    mask_test  = panel[\"scrape_date\"].isin(test_dates)\n",
    "    mask_val   = panel[\"scrape_date\"].isin(val_dates)\n",
    "    mask_train = ~(mask_test | mask_val)\n",
    "else:\n",
    "    order = panel[\"scrape_date\"].rank(method=\"first\")\n",
    "    q80 = order.quantile(0.8)\n",
    "    mask_train = order <= q80\n",
    "    mask_val   = (order > q80)\n",
    "    mask_test  = (order > q80)\n",
    "\n",
    "m_days = y_days.notna()\n",
    "m_disc = y_disc.notna()\n",
    "\n",
    "mask_tr_days  = (mask_train & m_days).to_numpy()\n",
    "mask_va_days  = (mask_val   & m_days).to_numpy()\n",
    "mask_te_days  = (mask_test  & m_days).to_numpy()\n",
    "\n",
    "mask_tr_disc  = (mask_train & m_disc).to_numpy()\n",
    "mask_va_disc  = (mask_val   & m_disc).to_numpy()\n",
    "mask_te_disc  = (mask_test  & m_disc).to_numpy()\n",
    "\n",
    "print(\"Days mask counts:\", mask_tr_days.sum(), mask_va_days.sum(), mask_te_days.sum())\n",
    "print(\"Pct  mask counts:\", mask_tr_disc.sum(), mask_va_disc.sum(), mask_te_disc.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f31a6200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_with_mask(X_df: pd.DataFrame, y_ser: pd.Series, mask: np.ndarray):\n",
    "    Xc = X_df[mask]\n",
    "    yc = y_ser[mask]\n",
    "\n",
    "    # ensure labels finite\n",
    "    yc = yc.replace([np.inf, -np.inf], np.nan)\n",
    "    m_y = np.isfinite(yc.to_numpy())\n",
    "    Xc = Xc.iloc[m_y]\n",
    "    yc = yc.iloc[m_y]\n",
    "\n",
    "    # XGBoost can handle NaN in X; just fix inf\n",
    "    Xc = Xc.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # cast to float32\n",
    "    Xc = Xc.astype(np.float32, copy=False)\n",
    "    yc = yc.astype(np.float32, copy=False)\n",
    "    return Xc, yc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5afaf4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean] days: (23011, 39) (95, 39) (593, 39)\n",
      "[clean] pct : (23011, 39) (95, 39) (593, 39)\n"
     ]
    }
   ],
   "source": [
    "# ===== MINIMAL TRAINING FALLBACK =====\n",
    "base_feats = [c for c in [\n",
    "    \"b_price\",\"item_price\",\"original_price\",\"b_unit_price\",\"item_unit_price\",\n",
    "    \"price_gap\",\"unit_price_gap\",\"discount_pct_filled\"\n",
    "] if c in panel.columns]\n",
    "\n",
    "# keep compact categorical codes if you created them; otherwise skip\n",
    "code_cols = [c for c in panel.columns if c.endswith(\"_code\")]\n",
    "base_feats += code_cols\n",
    "\n",
    "X_min   = panel[base_feats].astype(\"float32\")\n",
    "y_days2 = panel[\"y_days_to_next_discount\"]\n",
    "y_pct2  = panel[\"y_next_discount_pct\"]\n",
    "\n",
    "# keep rows where labels exist\n",
    "m_trainable_days = y_days2.notna()\n",
    "m_trainable_pct  = y_pct2.notna()\n",
    "\n",
    "# ===== REBUILD CLEAN MATRICES =====\n",
    "Xd_tr, yd_tr = sanitize_with_mask(X, y_days, mask_tr_days)\n",
    "Xd_va, yd_va = sanitize_with_mask(X, y_days, mask_va_days)\n",
    "Xd_te, yd_te = sanitize_with_mask(X, y_days, mask_te_days)\n",
    "\n",
    "Xr_tr, yr_tr = sanitize_with_mask(X, y_disc, mask_tr_disc)\n",
    "Xr_va, yr_va = sanitize_with_mask(X, y_disc, mask_va_disc)\n",
    "Xr_te, yr_te = sanitize_with_mask(X, y_disc, mask_te_disc)\n",
    "\n",
    "print(\"[clean] days:\", Xd_tr.shape, Xd_va.shape, Xd_te.shape)\n",
    "print(\"[clean] pct :\", Xr_tr.shape, Xr_va.shape, Xr_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ab8f1096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Time-to-next-discount]\n",
      " VAL : MAE=0.00 | R2=1.000 | n=95\n",
      " TEST: MAE=0.00 | R2=1.000 | n=593\n",
      "\n",
      "[Next-discount-%]\n",
      " VAL : MAE=7.93 | R2=-0.656 | n=95\n",
      " TEST: MAE=12.72 | R2=-0.740 | n=593\n"
     ]
    }
   ],
   "source": [
    "# ===== TRAIN XGBOOST =====\n",
    "xgb_days = XGBRegressor(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    tree_method=\"hist\", random_state=42\n",
    ")\n",
    "xgb_disc = XGBRegressor(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    tree_method=\"hist\", random_state=42\n",
    ")\n",
    "\n",
    "if len(yd_tr) == 0 or len(yr_tr) == 0:\n",
    "    raise RuntimeError(\"No training rows; check split or labels.\")\n",
    "\n",
    "xgb_days.fit(Xd_tr, yd_tr)\n",
    "xgb_disc.fit(Xr_tr, yr_tr)\n",
    "\n",
    "pred_days_va = np.clip(xgb_days.predict(Xd_va), 0, None) if len(yd_va) > 0 else np.array([])\n",
    "pred_days_te = np.clip(xgb_days.predict(Xd_te), 0, None) if len(yd_te) > 0 else np.array([])\n",
    "pred_disc_va = np.clip(xgb_disc.predict(Xr_va), 0, 100)  if len(yr_va) > 0 else np.array([])\n",
    "pred_disc_te = np.clip(xgb_disc.predict(Xr_te), 0, 100)  if len(yr_te) > 0 else np.array([])\n",
    "\n",
    "print(\"\\n[Time-to-next-discount]\")\n",
    "safe_eval(yd_va, pred_days_va, \" VAL \")\n",
    "safe_eval(yd_te, pred_days_te, \" TEST\")\n",
    "\n",
    "print(\"\\n[Next-discount-%]\")\n",
    "safe_eval(yr_va, pred_disc_va, \" VAL \")\n",
    "safe_eval(yr_te, pred_disc_te, \" TEST\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ead178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Time-to-next-discount]\n",
      " VAL : MAE=0.00 | R2=1.000 | n=95\n",
      " TEST: MAE=0.00 | R2=1.000 | n=593\n",
      "\n",
      "[Next-discount-%]\n",
      " VAL : MAE=7.93 | R2=-0.656 | n=95\n",
      " TEST: MAE=12.72 | R2=-0.740 | n=593\n"
     ]
    }
   ],
   "source": [
    "def safe_eval(y_true, y_pred, label):\n",
    "    n = len(y_true)\n",
    "    if n == 0:\n",
    "        print(f\"{label}: [no rows] — skipping metrics\")\n",
    "        return\n",
    "    try:\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        r2  = r2_score(y_true, y_pred)\n",
    "        print(f\"{label}: MAE={mae:.2f} | R2={r2:.3f} | n={n}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"{label}: could not compute metrics ({e}) | n={n}\")\n",
    "\n",
    "xgb_days = XGBRegressor(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    tree_method=\"hist\", random_state=42\n",
    ")\n",
    "xgb_disc = XGBRegressor(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    tree_method=\"hist\", random_state=42\n",
    ")\n",
    "\n",
    "if len(yd_tr) == 0 or len(yr_tr) == 0:\n",
    "    print(\"No training rows after cleaning; widen your date windows or revisit label logic.\")\n",
    "\n",
    "# fit\n",
    "xgb_days.fit(Xd_tr, yd_tr)\n",
    "xgb_disc.fit(Xr_tr, yr_tr)\n",
    "\n",
    "# predict (guard each split)\n",
    "pred_days_va = np.array([])\n",
    "pred_days_te = np.array([])\n",
    "pred_disc_va = np.array([])\n",
    "pred_disc_te = np.array([])\n",
    "\n",
    "if len(yd_va) > 0:\n",
    "    pred_days_va = np.clip(xgb_days.predict(Xd_va), 0, None)\n",
    "if len(yd_te) > 0:\n",
    "    pred_days_te = np.clip(xgb_days.predict(Xd_te), 0, None)\n",
    "\n",
    "if len(yr_va) > 0:\n",
    "    pred_disc_va = np.clip(xgb_disc.predict(Xr_va), 0, 100)\n",
    "if len(yr_te) > 0:\n",
    "    pred_disc_te = np.clip(xgb_disc.predict(Xr_te), 0, 100)\n",
    "\n",
    "print(\"\\n[Time-to-next-discount]\")\n",
    "safe_eval(yd_va, pred_days_va, \" VAL \")\n",
    "safe_eval(yd_te, pred_days_te, \" TEST\")\n",
    "\n",
    "print(\"\\n[Next-discount-%]\")\n",
    "safe_eval(yr_va, pred_disc_va, \" VAL \")\n",
    "safe_eval(yr_te, pred_disc_te, \" TEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4252aca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[alerts_best] rows: 97\n",
      "       sku                                               name  \\\n",
      "0  7531887  DentaStix Large Dog Treats Daily Oral Care Den...   \n",
      "1  6136311  Taste Toppers Dog Tin Multipack Chicken Select...   \n",
      "2  6856060  Dentastix Small Dog Treats Daily Oral Care Den...   \n",
      "3  4505916                Cat Food Pouch Mince & Jelly 12x80g   \n",
      "4  3452778       Peamutt-Butter Scrolls Chew Sticks Dog Treat   \n",
      "5  6824278  Grain Free Twice As Tasty Wet Cat Food Pate & ...   \n",
      "6  1853790                 Chicken Wrapped Rawhide Mini Bones   \n",
      "7   400326                           Original Clay Cat Litter   \n",
      "8  2951535  Earth Dog 2 In 1 Cherry Blossom Vanilla & Aloe...   \n",
      "9  8811550                                   Cat Litter Paper   \n",
      "\n",
      "   pred_days_to_next_discount  pred_next_discount_pct  \\\n",
      "0                         0.0               19.885164   \n",
      "1                         0.0               19.837505   \n",
      "2                         0.0               19.657907   \n",
      "3                         0.0               19.516720   \n",
      "4                         0.0               19.419174   \n",
      "5                         0.0               19.306393   \n",
      "6                         0.0               19.117016   \n",
      "7                         0.0               19.089424   \n",
      "8                         0.0               18.794477   \n",
      "9                         0.0               18.608629   \n",
      "\n",
      "                               alert  \n",
      "0  BUY — no strong discount imminent  \n",
      "1  BUY — no strong discount imminent  \n",
      "2  BUY — no strong discount imminent  \n",
      "3  BUY — no strong discount imminent  \n",
      "4  BUY — no strong discount imminent  \n",
      "5  BUY — no strong discount imminent  \n",
      "6  BUY — no strong discount imminent  \n",
      "7  BUY — no strong discount imminent  \n",
      "8  BUY — no strong discount imminent  \n",
      "9  BUY — no strong discount imminent  \n",
      "[SAVED alerts_best] C:\\Users\\pmayr\\Downloads\\Output\\alerts_latest_xgb_noleak.csv | rows: 97\n",
      "       sku                                               name  \\\n",
      "0  7531887  DentaStix Large Dog Treats Daily Oral Care Den...   \n",
      "1  6136311  Taste Toppers Dog Tin Multipack Chicken Select...   \n",
      "2  6856060  Dentastix Small Dog Treats Daily Oral Care Den...   \n",
      "3  4505916                Cat Food Pouch Mince & Jelly 12x80g   \n",
      "4  3452778       Peamutt-Butter Scrolls Chew Sticks Dog Treat   \n",
      "5  6824278  Grain Free Twice As Tasty Wet Cat Food Pate & ...   \n",
      "6  1853790                 Chicken Wrapped Rawhide Mini Bones   \n",
      "7   400326                           Original Clay Cat Litter   \n",
      "8  2951535  Earth Dog 2 In 1 Cherry Blossom Vanilla & Aloe...   \n",
      "9  8811550                                   Cat Litter Paper   \n",
      "\n",
      "   pred_days_to_next_discount  pred_next_discount_pct  \\\n",
      "0                         0.0               19.885164   \n",
      "1                         0.0               19.837505   \n",
      "2                         0.0               19.657907   \n",
      "3                         0.0               19.516720   \n",
      "4                         0.0               19.419174   \n",
      "5                         0.0               19.306393   \n",
      "6                         0.0               19.117016   \n",
      "7                         0.0               19.089424   \n",
      "8                         0.0               18.794477   \n",
      "9                         0.0               18.608629   \n",
      "\n",
      "                               alert  \n",
      "0  BUY — no strong discount imminent  \n",
      "1  BUY — no strong discount imminent  \n",
      "2  BUY — no strong discount imminent  \n",
      "3  BUY — no strong discount imminent  \n",
      "4  BUY — no strong discount imminent  \n",
      "5  BUY — no strong discount imminent  \n",
      "6  BUY — no strong discount imminent  \n",
      "7  BUY — no strong discount imminent  \n",
      "8  BUY — no strong discount imminent  \n",
      "9  BUY — no strong discount imminent  \n"
     ]
    }
   ],
   "source": [
    "# Define a folder to save outputs\n",
    "output_dir = Path(r\"C:/Users/pmayr/Downloads/Output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ===== ALERTS (latest snapshot, leak-free features) =====\n",
    "latest_date = panel[\"scrape_date\"].max()\n",
    "latest_rows = panel.loc[panel[\"scrape_date\"] == latest_date].copy()\n",
    "if len(latest_rows) == 0:\n",
    "    raise RuntimeError(\"No rows for latest_date.\")\n",
    "\n",
    "# Ensure the lag/roll columns exist for latest_rows (they were added to panel)\n",
    "X_latest = latest_rows.reindex(columns=X_cols, fill_value=np.nan).astype(\"float32\")\n",
    "\n",
    "pred_days_latest = np.clip(xgb_days.predict(X_latest), 0, None)\n",
    "pred_disc_latest = np.clip(xgb_disc.predict(X_latest), 0, 100)\n",
    "\n",
    "alerts_cols = [c for c in [\"sku\",\"name\",\"category\",\"season\",\"brand_tier\",\"size_band\",\"scrape_date\",\"item_price\",\"original_price\"] if c in latest_rows.columns]\n",
    "alerts = latest_rows[alerts_cols].copy()\n",
    "alerts[\"pred_days_to_next_discount\"] = pred_days_latest\n",
    "alerts[\"pred_next_discount_pct\"]     = pred_disc_latest\n",
    "\n",
    "# === Clean & prioritise alerts ===\n",
    "alerts_clean = alerts.copy()\n",
    "\n",
    "# (a) Drop rows with clearly invalid price info for the latest snapshot\n",
    "if {\"item_price\",\"original_price\"}.issubset(alerts_clean.columns):\n",
    "    bad_price = (alerts_clean[\"item_price\"].fillna(0) == 0) & (alerts_clean[\"original_price\"].fillna(0) == 0)\n",
    "    alerts_clean = alerts_clean.loc[~bad_price].copy()\n",
    "\n",
    "# (b) Build a simple priority score:\n",
    "#     - Higher predicted discount% is better\n",
    "#     - Sooner predicted days is better\n",
    "alerts_clean[\"priority\"] = (\n",
    "    (alerts_clean[\"pred_next_discount_pct\"].fillna(0) / 100).clip(0, 1) * 0.6\n",
    "    + ((7 - alerts_clean[\"pred_days_to_next_discount\"].fillna(999)).clip(lower=0) / 7) * 0.4\n",
    ")\n",
    "\n",
    "# (c) Keep ONE row per SKU (the highest-priority recommendation)\n",
    "alerts_best = (\n",
    "    alerts_clean\n",
    "    .sort_values([\"priority\", \"pred_next_discount_pct\"], ascending=False)\n",
    "    .drop_duplicates(subset=[\"sku\"], keep=\"first\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# (d) Recompute the alert label on the deduped frame (optional but tidy)\n",
    "ALERT_DAY_THRESHOLD  = 7\n",
    "ALERT_DISC_THRESHOLD = 20.0\n",
    "alerts_best[\"alert\"] = np.where(\n",
    "    (alerts_best[\"pred_days_to_next_discount\"] <= ALERT_DAY_THRESHOLD) &\n",
    "    (alerts_best[\"pred_next_discount_pct\"]   >= ALERT_DISC_THRESHOLD),\n",
    "    \"HOLD — high chance of discount soon\",\n",
    "    \"BUY — no strong discount imminent\"\n",
    ")\n",
    "\n",
    "# (e) Sort for presentation: HOLD first, then by priority\n",
    "alerts_best = alerts_best.sort_values(\n",
    "    [\"alert\", \"priority\", \"pred_next_discount_pct\", \"pred_days_to_next_discount\"],\n",
    "    ascending=[True, False, False, True]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Quick peek\n",
    "print(\"[alerts_best] rows:\", len(alerts_best))\n",
    "print(alerts_best.head(10)[[\"sku\",\"name\",\"pred_days_to_next_discount\",\"pred_next_discount_pct\",\"alert\"]])\n",
    "\n",
    "# Save the deduped & prioritised alerts (this HAS the 'alert' column)\n",
    "alerts_path = output_dir / \"alerts_latest_xgb_noleak.csv\"\n",
    "alerts_best.to_csv(alerts_path, index=False)\n",
    "print(\"[SAVED alerts_best]\", alerts_path, \"| rows:\", len(alerts_best))\n",
    "\n",
    "# Quick preview\n",
    "print(\n",
    "    alerts_best.head(10)[\n",
    "        [\"sku\",\"name\",\"pred_days_to_next_discount\",\"pred_next_discount_pct\",\"alert\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea861965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[banned ∩ X] = []\n"
     ]
    }
   ],
   "source": [
    "# Columns that must NOT be in X (same-day label proxies)\n",
    "banned = {\n",
    "    \"discount_percentage\",\"discount_pct_filled\",\"is_on_promo\",\n",
    "    \"has_discount_text\",\"has_promo_text\"\n",
    "}\n",
    "print(\"[banned ∩ X] =\", sorted(list(set(X.columns) & banned)))\n",
    "assert len(set(X.columns) & banned) == 0, \"Leakage: X includes same-day label fields!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb89c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[BASELINES]\n",
      "VAL  days→cat-median: MAE=0.00 | R2=1.000 | n=95\n",
      "TEST days→cat-median: MAE=0.00 | R2=1.000 | n=593\n",
      "VAL   pct→cat-median: MAE=3.02 | R2=-0.224 | n=95\n",
      "TEST  pct→cat-median: MAE=15.25 | R2=-1.315 | n=593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def cat_median_baseline(panel, y_col, key_cols, train_mask, target_mask):\n",
    "    # Training slice\n",
    "    tr = panel.loc[train_mask & panel[y_col].notna(), key_cols + [y_col]].copy()\n",
    "    if tr.empty:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    global_med = tr[y_col].median()\n",
    "\n",
    "    # Median per key (pass observed=False to silence future warning)\n",
    "    med = (\n",
    "        tr.groupby(key_cols, dropna=False, observed=False)[y_col]\n",
    "          .median()\n",
    "          .rename(\"pred\")\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    # Target slice (VAL or TEST)\n",
    "    tgt = panel.loc[target_mask, key_cols + [y_col]].copy()\n",
    "    tgt = tgt.merge(med, on=key_cols, how=\"left\")\n",
    "\n",
    "    # Fill missing predictions with global median\n",
    "    tgt[\"pred\"] = tgt[\"pred\"].fillna(global_med)\n",
    "\n",
    "    # Drop rows where y_true is NaN before metrics\n",
    "    m = tgt[y_col].notna().to_numpy()\n",
    "    y_true = tgt.loc[m, y_col].to_numpy()\n",
    "    y_pred = tgt.loc[m, \"pred\"].to_numpy()\n",
    "\n",
    "    return y_pred, y_true\n",
    "\n",
    "def show_baseline(label, y_true, y_pred):\n",
    "    n = len(y_true)\n",
    "    if n == 0:\n",
    "        print(f\"{label}: [no rows]\")\n",
    "        return\n",
    "    # guard: remove any lingering NaNs/Infs (shouldn't happen, but safe)\n",
    "    m = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "    y_true, y_pred = y_true[m], y_pred[m]\n",
    "    if len(y_true) == 0:\n",
    "        print(f\"{label}: [no valid rows after NaN/Inf filtering]\")\n",
    "        return\n",
    "    print(f\"{label}: MAE={mean_absolute_error(y_true, y_pred):.2f} | R2={r2_score(y_true, y_pred):.3f} | n={len(y_true)}\")\n",
    "\n",
    "# -------- run baselines --------\n",
    "keys = [\"category\",\"brand_tier\"]  # you can try [\"category\"] alone as a variant\n",
    "\n",
    "# DAYS baseline\n",
    "b_pred_va_days, b_true_va_days = cat_median_baseline(panel, \"y_days_to_next_discount\", keys, mask_train, mask_val)\n",
    "b_pred_te_days, b_true_te_days = cat_median_baseline(panel, \"y_days_to_next_discount\", keys, mask_train, mask_test)\n",
    "\n",
    "# PCT baseline\n",
    "b_pred_va_pct,  b_true_va_pct  = cat_median_baseline(panel, \"y_next_discount_pct\", keys, mask_train, mask_val)\n",
    "b_pred_te_pct,  b_true_te_pct  = cat_median_baseline(panel, \"y_next_discount_pct\", keys, mask_train, mask_test)\n",
    "\n",
    "print(\"\\n[BASELINES]\")\n",
    "show_baseline(\"VAL  days→cat-median\", b_true_va_days, b_pred_va_days)\n",
    "show_baseline(\"TEST days→cat-median\", b_true_te_days, b_pred_te_days)\n",
    "show_baseline(\"VAL   pct→cat-median\", b_true_va_pct,  b_pred_va_pct)\n",
    "show_baseline(\"TEST  pct→cat-median\", b_true_te_pct,  b_pred_te_pct)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
