{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe746c31",
   "metadata": {},
   "source": [
    "# Deal Prediction Modelling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542d1dd2",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "438618cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install xgboost\n",
    "\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from pathlib import Path\n",
    "import sys, subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9bec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "output = Path(r\"C:/Users/pmayr/Downloads/Output\")\n",
    "data_path = output/\"staged_features_events_brands_size.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77cb04d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmayr\\AppData\\Local\\Temp\\ipykernel_20840\\1617873353.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(data_path)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "if \"scrape_date\" not in df.columns:\n",
    "    df[\"scrape_date\"] = pd.to_datetime(df[\"scrape_date_str\"], errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8dc9873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47503, 36)        sku scrape_date category\n",
      "0  8371390  2025-04-02   Easter\n",
      "1  7473849  2025-04-02   Easter\n",
      "2  5726070  2025-04-02   Easter\n"
     ]
    }
   ],
   "source": [
    "#Keeping only yhe rows withe the sku and  date\n",
    "df = df.dropna(subset=[\"sku\", \"scrape_date\"]).copy()\n",
    "\n",
    "# fill discounts safely\n",
    "if \"discount_percentage\" in df.columns:\n",
    "    df[\"discount_percentage\"] = df[\"discount_percentage\"].fillna(0.0)\n",
    "if \"discount_pct_filled\" in df.columns:\n",
    "    df[\"discount_pct_filled\"] = df[\"discount_pct_filled\"].fillna(0.0)\n",
    "\n",
    "# cast some categoricals (optional)\n",
    "for col in [\"brand_tier\", \"size_band\", \"season\", \"category\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "print(df.shape, df[[\"sku\",\"scrape_date\",\"category\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "244e105e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels] non-null rates: days 0.49889480664379093 | pct 0.49889480664379093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pmayr\\AppData\\Local\\Temp\\ipykernel_20840\\382189985.py:43: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  panel = df.groupby(\"sku\", group_keys=False).apply(build_forward_labels_with_fallback)\n"
     ]
    }
   ],
   "source": [
    "# ===== LABELS with fallback to next observation =====\n",
    "def build_forward_labels_with_fallback(sub: pd.DataFrame) -> pd.DataFrame:\n",
    "    sub = sub.sort_values(\"scrape_date\").reset_index(drop=True)\n",
    "    n = len(sub)\n",
    "    next_days = np.full(n, np.nan, dtype=float)\n",
    "    next_pct  = np.full(n, np.nan, dtype=float)\n",
    "\n",
    "    # Define promo condition robustly\n",
    "    has_disc_col = \"discount_pct_filled\" in sub.columns\n",
    "    on_promo = (\n",
    "        (sub[\"is_on_promo\"] == 1)\n",
    "        | (sub[\"discount_percentage\"].fillna(0) > 0 if \"discount_percentage\" in sub.columns else False)\n",
    "        | (sub[\"discount_pct_filled\"].fillna(0) > 0 if has_disc_col else False)\n",
    "    ).to_numpy()\n",
    "\n",
    "    promo_idx = np.where(on_promo)[0]\n",
    "\n",
    "    for i in range(n - 1):  # last row canâ€™t have a future label\n",
    "        # 1) try next promo strictly after i\n",
    "        j_candidates = promo_idx[promo_idx > i]\n",
    "        if len(j_candidates) > 0:\n",
    "            j = j_candidates[0]\n",
    "        else:\n",
    "            # 2) fallback to the very next observation\n",
    "            j = i + 1\n",
    "\n",
    "        # days until j\n",
    "        next_days[i] = (sub.loc[j, \"scrape_date\"] - sub.loc[i, \"scrape_date\"]).days\n",
    "\n",
    "        # discount % at j (prefer filled)\n",
    "        if has_disc_col and pd.notna(sub.loc[j, \"discount_pct_filled\"]):\n",
    "            next_pct[i] = float(sub.loc[j, \"discount_pct_filled\"])\n",
    "        elif \"discount_percentage\" in sub.columns and pd.notna(sub.loc[j, \"discount_percentage\"]):\n",
    "            next_pct[i] = float(sub.loc[j, \"discount_percentage\"])\n",
    "        else:\n",
    "            next_pct[i] = 0.0  # safe fallback\n",
    "\n",
    "    sub[\"y_days_to_next_discount\"] = next_days\n",
    "    sub[\"y_next_discount_pct\"]     = next_pct\n",
    "    return sub\n",
    "\n",
    "# rebuild panel with the new labels\n",
    "panel = df.groupby(\"sku\", group_keys=False).apply(build_forward_labels_with_fallback)\n",
    "print(\"[labels] non-null rates:\",\n",
    "      \"days\", panel[\"y_days_to_next_discount\"].notna().mean(),\n",
    "      \"| pct\", panel[\"y_next_discount_pct\"].notna().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1fe494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X shape] (47503, 13)\n",
      "Categorical codes added: ['brand_tier_code', 'size_band_code', 'season_code', 'category_code']\n"
     ]
    }
   ],
   "source": [
    "# ===== 4) FEATURES: NUMERIC + LAGS + COMPACT CAT CODES (no one-hot) =====\n",
    "# numeric features to use\n",
    "num_cols = [c for c in [\n",
    "    \"b_price\",\"item_price\",\"original_price\",\"b_unit_price\",\"item_unit_price\",\n",
    "    \"price_gap\",\"unit_price_gap\",\n",
    "    \"discount_pct_filled\"\n",
    "] if c in panel.columns]\n",
    "\n",
    "# create lags\n",
    "panel = panel.sort_values([\"sku\",\"scrape_date\"]).copy()\n",
    "if \"item_price\" in panel.columns:\n",
    "    panel[\"item_price_lag1\"]    = panel.groupby(\"sku\")[\"item_price\"].shift(1)\n",
    "    panel[\"pct_chg_item_price\"] = panel[\"item_price\"] / panel[\"item_price_lag1\"] - 1.0\n",
    "    num_cols += [\"item_price_lag1\",\"pct_chg_item_price\"]\n",
    "\n",
    "if \"discount_pct_filled\" in panel.columns:\n",
    "    panel[\"disc_pct_filled_lag1\"] = panel.groupby(\"sku\")[\"discount_pct_filled\"].shift(1)\n",
    "    num_cols += [\"disc_pct_filled_lag1\"]\n",
    "\n",
    "# ---- compact categorical encoding ----\n",
    "cat_cols = [c for c in [\"brand_tier\",\"size_band\",\"season\",\"category\"] if c in panel.columns]\n",
    "\n",
    "def add_compact_codes(df, col, top_k=30):\n",
    "    # keep only top_k frequent levels; others -> \"OTHER\"\n",
    "    s = df[col].astype(str)\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    keep = set(vc.head(top_k).index)\n",
    "    safe = s.where(s.isin(keep), \"OTHER\").astype(\"category\")\n",
    "    code_col = f\"{col}_code\"\n",
    "    df[code_col] = safe.cat.codes.astype(\"int16\")   # compact integer code\n",
    "    return code_col\n",
    "\n",
    "code_cols = []\n",
    "for c in cat_cols:\n",
    "    code_cols.append(add_compact_codes(panel, c, top_k=30))\n",
    "\n",
    "num_cols += code_cols\n",
    "\n",
    "# build X/Y with compact numeric features only (no get_dummies)\n",
    "X = panel[num_cols].astype(\"float32\")   # float32 keeps memory low\n",
    "y_days = panel[\"y_days_to_next_discount\"]\n",
    "y_disc = panel[\"y_next_discount_pct\"]\n",
    "\n",
    "# sanity\n",
    "assert X.index.equals(panel.index), \"X and panel indices must match.\"\n",
    "print(\"[X shape]\", X.shape)\n",
    "print(\"Categorical codes added:\", code_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35ec39f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days mask counts: 6282 36 125\n",
      "Pct  mask counts: 6282 36 125\n"
     ]
    }
   ],
   "source": [
    "# ===== 5) TEMPORAL SPLIT =====\n",
    "last_date   = panel[\"scrape_date\"].max()\n",
    "val_cutoff  = last_date - pd.Timedelta(days=14)\n",
    "test_cutoff = last_date - pd.Timedelta(days=7)\n",
    "\n",
    "mask_train = panel[\"scrape_date\"] <  val_cutoff\n",
    "mask_val   = (panel[\"scrape_date\"] >= val_cutoff) & (panel[\"scrape_date\"] < test_cutoff)\n",
    "mask_test  = panel[\"scrape_date\"] >= test_cutoff\n",
    "\n",
    "m_days = y_days.notna()\n",
    "m_disc = y_disc.notna()\n",
    "\n",
    "# indices (aligned to panel.index)\n",
    "mask_tr_days  = (mask_train & m_days).to_numpy()\n",
    "mask_va_days  = (mask_val   & m_days).to_numpy()\n",
    "mask_te_days  = (mask_test  & m_days).to_numpy()\n",
    "\n",
    "mask_tr_disc  = (mask_train & m_disc).to_numpy()\n",
    "mask_va_disc  = (mask_val   & m_disc).to_numpy()\n",
    "mask_te_disc  = (mask_test  & m_disc).to_numpy()\n",
    "\n",
    "print(\"Days mask counts:\",\n",
    "      mask_tr_days.sum(), mask_va_days.sum(), mask_te_days.sum())\n",
    "print(\"Pct  mask counts:\",\n",
    "      mask_tr_disc.sum(), mask_va_disc.sum(), mask_te_disc.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "702102a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days (clean): (0, 13) (0, 13) (0, 13)\n",
      "Pct  (clean): (0, 13) (0, 13) (0, 13)\n"
     ]
    }
   ],
   "source": [
    "def sanitize_with_mask(X_df: pd.DataFrame, y_ser: pd.Series, mask: np.ndarray):\n",
    "    # 1) quick boolean slice by mask (no index alignment cost)\n",
    "    Xc = X_df[mask]\n",
    "    yc = y_ser[mask]\n",
    "\n",
    "    # 2) drop rows with non-finite labels\n",
    "    yv = yc.to_numpy()\n",
    "    m_y = np.isfinite(yv)\n",
    "    Xc = Xc.iloc[m_y]\n",
    "    yc = yc.iloc[m_y]\n",
    "\n",
    "    # 3) now cast to float32 (AFTER filtering to save memory)\n",
    "    Xc = Xc.astype(np.float32, copy=False)\n",
    "    yc = yc.astype(np.float32, copy=False)\n",
    "\n",
    "    # 4) replace inf -> NaN then drop any row with NaN/inf in X\n",
    "    Xc = Xc.replace([np.inf, -np.inf], np.nan)\n",
    "    yc = yc.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    mX = np.isfinite(Xc.to_numpy()).all(axis=1)\n",
    "    Xc = Xc.iloc[mX]\n",
    "    yc = yc.iloc[mX]\n",
    "\n",
    "    # 5) safety\n",
    "    assert np.isfinite(yc.to_numpy()).all(), \"Label still non-finite.\"\n",
    "    assert np.isfinite(Xc.to_numpy()).all(), \"Features still non-finite.\"\n",
    "    return Xc, yc\n",
    "\n",
    "# Build clean matrices using masks (NO .loc with giant indexers)\n",
    "Xd_tr, yd_tr = sanitize_with_mask(X, y_days, mask_tr_days)\n",
    "Xd_va, yd_va = sanitize_with_mask(X, y_days, mask_va_days)\n",
    "Xd_te, yd_te = sanitize_with_mask(X, y_days, mask_te_days)\n",
    "\n",
    "Xr_tr, yr_tr = sanitize_with_mask(X, y_disc, mask_tr_disc)\n",
    "Xr_va, yr_va = sanitize_with_mask(X, y_disc, mask_va_disc)\n",
    "Xr_te, yr_te = sanitize_with_mask(X, y_disc, mask_te_disc)\n",
    "\n",
    "print(\"Days (clean):\", Xd_tr.shape, Xd_va.shape, Xd_te.shape)\n",
    "print(\"Pct  (clean):\", Xr_tr.shape, Xr_va.shape, Xr_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96c47995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No training rows after cleaning; check label creation or split windows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pmayr\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:03:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:52: Empty dataset at worker: 0\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Time-to-next-discount]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m pred_days_te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(xgb_days\u001b[38;5;241m.\u001b[39mpredict(Xd_te), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Time-to-next-discount]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m VAL  MAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_absolute_error(yd_va, pred_days_va)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     25\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| R2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2_score(yd_va, pred_days_va)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m TEST MAE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_absolute_error(yd_te, pred_days_te)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m),\n\u001b[0;32m     27\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m| R2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, r2_score(yd_te, pred_days_te)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     29\u001b[0m xgb_disc\u001b[38;5;241m.\u001b[39mfit(Xr_tr, yr_tr)\n",
      "File \u001b[1;32mc:\\Users\\pmayr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pmayr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:216\u001b[0m, in \u001b[0;36mmean_absolute_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    153\u001b[0m     {\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m ):\n\u001b[0;32m    164\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean absolute error regression loss.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_absolute_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    0.85...\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    217\u001b[0m         y_true, y_pred, multioutput\n\u001b[0;32m    218\u001b[0m     )\n\u001b[0;32m    219\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    220\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage(np\u001b[38;5;241m.\u001b[39mabs(y_pred \u001b[38;5;241m-\u001b[39m y_true), weights\u001b[38;5;241m=\u001b[39msample_weight, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pmayr\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:112\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype, xp)\u001b[0m\n\u001b[0;32m    109\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred, multioutput, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    111\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m--> 112\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    113\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\pmayr\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1091\u001b[0m         )\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# ===== 7) TRAIN & EVALUATE XGBOOST =====\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "xgb_days = XGBRegressor(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    tree_method=\"hist\", random_state=42\n",
    ")\n",
    "xgb_disc = XGBRegressor(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "    tree_method=\"hist\", random_state=42\n",
    ")\n",
    "\n",
    "if len(yd_tr) == 0 or len(yr_tr) == 0:\n",
    "    print(\"No training rows after cleaning; check label creation or split windows.\")\n",
    "\n",
    "xgb_days.fit(Xd_tr, yd_tr)\n",
    "pred_days_va = np.clip(xgb_days.predict(Xd_va), 0, None)\n",
    "pred_days_te = np.clip(xgb_days.predict(Xd_te), 0, None)\n",
    "\n",
    "print(\"\\n[Time-to-next-discount]\")\n",
    "print(\" VAL  MAE:\", mean_absolute_error(yd_va, pred_days_va).round(2),\n",
    "      \"| R2:\", r2_score(yd_va, pred_days_va).round(3))\n",
    "print(\" TEST MAE:\", mean_absolute_error(yd_te, pred_days_te).round(2),\n",
    "      \"| R2:\", r2_score(yd_te, pred_days_te).round(3))\n",
    "\n",
    "xgb_disc.fit(Xr_tr, yr_tr)\n",
    "pred_disc_va = np.clip(xgb_disc.predict(Xr_va), 0, 100)\n",
    "pred_disc_te = np.clip(xgb_disc.predict(Xr_te), 0, 100)\n",
    "\n",
    "print(\"\\n[Next-discount-%]\")\n",
    "print(\" VAL  MAE:\", mean_absolute_error(yr_va, pred_disc_va).round(2),\n",
    "      \"| R2:\", r2_score(yr_va, pred_disc_va).round(3))\n",
    "print(\" TEST MAE:\", mean_absolute_error(yr_te, pred_disc_te).round(2),\n",
    "      \"| R2:\", r2_score(yr_te, pred_disc_te).round(3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
