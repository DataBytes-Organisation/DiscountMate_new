{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76a5515b-8704-48d8-b741-ccfe5dc5c167",
   "metadata": {},
   "source": [
    "# Smart Substitution Model\n",
    "\n",
    "**DataBytes - DiscountMate Project - ML Team**\n",
    "\n",
    "**Author:**\n",
    "Bailey Mulcahy (224534798), s224534798@deakin.edu.au <br>\n",
    "\n",
    "---\n",
    "## Overview\n",
    "\n",
    "This document outlines a suggested approach for the smart substitution model, including the following sections:\n",
    "\n",
    "* **Introduction**\n",
    "* **Loading the Dataset**\n",
    "* **Preprocessing the Data**\n",
    "* **Building the Model**\n",
    "* **Evaluating the Model**\n",
    "* **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543671c4-8c5b-4ac1-815f-1cfb70e2bba7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The aim of the smart substitution model is to suggest similar items of similar quality and size, that are cheaper than the original. For example, it may suggest a \\\\$4 carton of milk instead of a \\\\$5 carton that is the same size.\n",
    "\n",
    "This document creates the model and generates suggested items to substitute for all original items in the dataset. A new dataset is created, containing the original information, as well as information for the suggested substitute item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2296748-ac9f-4526-b205-582e26febbcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading the Dataset\n",
    "\n",
    "This first section loads the dataset as a pandas DataFrame and prints the first row to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e259c4-fb05-4da9-bf25-235524ac6c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   product_code                                    name  brand  \\\n",
      "0       8371390  coles hot cross buns traditional fruit  coles   \n",
      "\n",
      "  brand_confidence brand_tier category subcategory  original_price  \\\n",
      "0      store_brand      store   easter      easter             4.4   \n",
      "\n",
      "   sale_price  std_item_size std_item_size_unit  item_size  price_per_unit  \\\n",
      "0         3.0            6.0               pack     6.0274            0.73   \n",
      "\n",
      "  unit_type size_band                                 tags  similarity_score  \n",
      "0      each     mixed  ['coles', 'cross', 'buns', 'fruit']          0.179609  \n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.sparse import hstack\n",
    "import ast\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Load the dataset as a pandas DataFrame and print the first row\n",
    "df = pd.read_csv(\"smart_substitution_dataset.csv\")\n",
    "print(df.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c459f79c-3874-42bc-bdfc-c5814c976858",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Preprocessing the Data\n",
    "\n",
    "This section involves fixing some issues that may lead to poor predictions, as well as preparing the dataset for TF-IDF. This includes filling missing numeric columns and combining features for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad5c750f-cda1-47d4-85a3-fadb59e8dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and preprocessed. Total rows: 24897\n"
     ]
    }
   ],
   "source": [
    "# Convert tags from string to list\n",
    "if isinstance(df['tags'].iloc[0], str):             # If 'tags' is a string,\n",
    "    df['tags'] = df['tags'].apply(ast.literal_eval) # Convert to list\n",
    "\n",
    "# Combine tags into a single string for TF-IDF (space separated)\n",
    "df['tags_str'] = df['tags'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Fill missing numeric columns\n",
    "df['item_size'] = df['item_size'].fillna(1) # Default to 1\n",
    "df['price_per_unit'] = df['price_per_unit'].fillna(df['sale_price'] / df['item_size']) # Fill missing values with sale price divided by item size\n",
    "\n",
    "# Combine features for similarity into one string (tags + category + subcategory)\n",
    "df['combined_features'] = df['tags_str'] + ' ' + df['category'] + ' ' + df['subcategory']\n",
    "\n",
    "print(f\"Data loaded and preprocessed. Total rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee59f5d-a119-4750-993c-f86ae7b71776",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Building the Model\n",
    "\n",
    "This section builds a nearest neighbors model and prints an update on the item generating process after every 1000 rows.\n",
    "\n",
    "**How does this model work?**\n",
    "\n",
    "* The model begins by converting text (the combined string of tags + category + subcategory) into a TF-IDF matrix, where each product is represented as a numerical vector of word/term weights. stop_words='english' ensures common words like \"the\", \"and\", or \"of\" are ignored.\n",
    "* Numeric features (price_per_unit and item_size) are scaled using StandardScaler to ensure they are comparable with other features, and categorical features (size_band, category, and subcategory) are one-hot encoded.\n",
    "* These text, numeric, and categorical vectors are combined into a hybrid feature matrix for each product, allowing the model to capture both semantic and structured feature similarity.\n",
    "* A NearestNeighbors model is created using cosine distance, which measures similarity between the hybrid vectors. n_neighbors=6 retrieves the 5 nearest neighbors plus the item itself.\n",
    "* The dataset is prepared to store suggested items by creating new columns for the recommendations.\n",
    "* The model loops through every item to generate substitutes. For each item, it finds the 6 nearest neighbors in the hybrid feature space and converts cosine distance to similarity (closer to 1 = more similar).\n",
    "* Cost-focused filtering is applied: Only neighbors that are cheaper than the current item are considered.\n",
    "* Among the cheaper candidates, the most similar one is selected. If no cheaper neighbor exists, no recommendation is made for that item.\n",
    "* Savings are calculated as the difference between the original item's sale price and the suggested substituteâ€™s sale price, and similarity is recorded.\n",
    "* Finally, the information of the selected substitute is added to the dataset, and a new CSV file is created with all recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1225fb20-6586-4bab-8b11-076a7fce9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid NearestNeighbors model built (TF-IDF + numeric + categorical).\n",
      "Processed 1000/24897 rows.\n",
      "Processed 2000/24897 rows.\n",
      "Processed 3000/24897 rows.\n",
      "Processed 4000/24897 rows.\n",
      "Processed 5000/24897 rows.\n",
      "Processed 6000/24897 rows.\n",
      "Processed 7000/24897 rows.\n",
      "Processed 8000/24897 rows.\n",
      "Processed 9000/24897 rows.\n",
      "Processed 10000/24897 rows.\n",
      "Processed 11000/24897 rows.\n",
      "Processed 12000/24897 rows.\n",
      "Processed 13000/24897 rows.\n",
      "Processed 14000/24897 rows.\n",
      "Processed 15000/24897 rows.\n",
      "Processed 16000/24897 rows.\n",
      "Processed 17000/24897 rows.\n",
      "Processed 18000/24897 rows.\n",
      "Processed 19000/24897 rows.\n",
      "Processed 20000/24897 rows.\n",
      "Processed 21000/24897 rows.\n",
      "Processed 22000/24897 rows.\n",
      "Processed 23000/24897 rows.\n",
      "Processed 24000/24897 rows.\n",
      "Processed 24897/24897 rows.\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Build NearestNeighbors Model\n",
    "# -----------------------------\n",
    "\n",
    "# Convert text (combined feature string) to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_features'])\n",
    "\n",
    "# Scale numeric features\n",
    "numeric_features = df[['price_per_unit', 'item_size']].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(numeric_features)\n",
    "numeric_sparse = csr_matrix(numeric_scaled)\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_features = df[['size_band', 'category', 'subcategory']].fillna(\"unknown\")\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "categorical_encoded = encoder.fit_transform(categorical_features)\n",
    "\n",
    "# Combine TF-IDF, numeric features, and categorical features into a hybrid matrix\n",
    "hybrid_matrix = hstack([tfidf_matrix, numeric_sparse, categorical_encoded])\n",
    "\n",
    "# Fit NearestNeighbors model (cosine distance)\n",
    "nn_model = NearestNeighbors(n_neighbors=6, metric='cosine', n_jobs=-1)  # Retrieves 5 similar products + itself\n",
    "nn_model.fit(hybrid_matrix) # Train NearestNeighbors model on the TF-IDF vectors\n",
    "\n",
    "print(\"Hybrid NearestNeighbors model built (TF-IDF + numeric + categorical).\")\n",
    "\n",
    "# --------------------------\n",
    "# Generate Substitute Items\n",
    "# --------------------------\n",
    "\n",
    "# List of all original columns to copy for suggested items\n",
    "original_cols = [\n",
    "    'product_code', 'name', 'brand', 'brand_confidence', 'brand_tier',\n",
    "    'category', 'subcategory', 'original_price', 'sale_price',\n",
    "    'std_item_size', 'std_item_size_unit', 'item_size', 'price_per_unit',\n",
    "    'unit_type', 'size_band', 'tags', 'similarity_score'\n",
    "]\n",
    "\n",
    "# Prepare new columns with 'suggested_' prefix\n",
    "for col in original_cols:\n",
    "    df[f'suggested_{col}'] = None\n",
    "\n",
    "# Explicitly create suggested_savings and suggested_similarity\n",
    "df['suggested_savings'] = None\n",
    "df['suggested_similarity'] = None\n",
    "\n",
    "# Loop through each item\n",
    "total_rows = len(df)\n",
    "for idx in range(total_rows):\n",
    "    \n",
    "    # Find nearest neighbors\n",
    "    distances, indices = nn_model.kneighbors(hybrid_matrix[idx:idx+1]) # Finds 6 nearest neighbors for current item\n",
    "    similarities = 1 - distances[0] # Convert cosine distance to similarity (closer to 1 = more similar)\n",
    "    candidate_idxs = indices[0][1:] # Skip self (drop the first neighbor)\n",
    "    similarities_candidates = similarities[1:] # Skip self for similarities\n",
    "\n",
    "    # Select the most similar candidate\n",
    "    if len(candidate_idxs) > 0:\n",
    "        # Identify all cheaper candidates\n",
    "        cheaper_mask = df.iloc[candidate_idxs]['sale_price'].values < df.iloc[idx]['sale_price']\n",
    "        \n",
    "        if cheaper_mask.any():\n",
    "            # Select the most similar among cheaper candidates\n",
    "            cheaper_candidates = candidate_idxs[cheaper_mask]\n",
    "            cheaper_sims = similarities_candidates[cheaper_mask]\n",
    "            best_pos = cheaper_sims.argmax()\n",
    "            best_idx = cheaper_candidates[best_pos]\n",
    "            best_similarity = cheaper_sims[best_pos]\n",
    "\n",
    "            # Save all suggested item details\n",
    "            for col in original_cols:\n",
    "                df.at[idx, f'suggested_{col}'] = df.iloc[best_idx][col]\n",
    "            \n",
    "            # Calculate savings\n",
    "            savings = df.iloc[idx]['sale_price'] - df.iloc[best_idx]['sale_price']\n",
    "            \n",
    "            # Save extra info\n",
    "            df.at[idx, 'suggested_savings'] = round(savings, 2)\n",
    "            df.at[idx, 'suggested_similarity'] = round(best_similarity, 2)\n",
    "\n",
    "    # Progress update every 1000 rows\n",
    "    if (idx + 1) % 1000 == 0 or idx == total_rows - 1:\n",
    "        print(f\"Processed {idx + 1}/{total_rows} rows.\")\n",
    "\n",
    "# Save results to CSV\n",
    "df.to_csv('smart_substitution_with_recommendations.csv', index=False)\n",
    "print('File saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece28bc8-f87f-40d8-b808-12ab7e370fe2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluating the Model\n",
    "\n",
    "There are a few key evaluation metrics that can be used to give an overview of how the model is performing. What we want to see is firstly whether the model is saving money by suggesting cheaper alternatives to items, and additionally how similar the item suggestions actually are. It is crucial that these metrics are used in combination with each other, as they do not give a strong picture of model performance when used alone. For example, if we were to use just 'money saved' as a metric, a model that simply suggests the cheapest item available would beat all other models - this is not what we want. We want a model that suggests cheaper alternatives that *are very similar* to the original item. Therefore, if we use other metrics like swap acceptance rate (the percentage of suggested items that meet specified conditions), we can get a good overall view of both the money saved, and the similarity of the items to the original ones.\n",
    "\n",
    "**Number of substitutions suggested:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e2c9bb2-0996-4b56-900b-d1474cbca2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items with no suggestion: 6623\n",
      "Items with a suggestion: 18274\n",
      "Percentage of items with a suggestion: 73.40%\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV with recommendations\n",
    "df_results = pd.read_csv(\"smart_substitution_with_recommendations.csv\")\n",
    "\n",
    "# Count null and non-null values\n",
    "missing_count = df_results['suggested_product_code'].isnull().sum()\n",
    "non_missing_count = df_results['suggested_product_code'].notnull().sum()\n",
    "\n",
    "# Calculate percentage of items with a suggestion\n",
    "percent_with_suggestion = (non_missing_count / len(df_results)) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Items with no suggestion: {missing_count}\")\n",
    "print(f\"Items with a suggestion: {non_missing_count}\")\n",
    "print(f\"Percentage of items with a suggestion: {percent_with_suggestion:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955cc7a4-934a-43e4-bad9-f03180b7c497",
   "metadata": {},
   "source": [
    "As we can see above, there were 6623 original items that did not receive a substitution suggestion. This means that out of the top 5 most similar items to the original item, none of them were cheaper. The logic of the model is dependent on the assumption that it should not be recommending any items that are not cheaper than the original, which goes with the 'smart substitution' idea. However, this could be changed if necessary, and it could be set so that every single item receives a recommendation - the issue is, these recommendations wouldn't always be cheaper. Overall, 73% of items received a suggested substitute at a cheaper price.\n",
    "\n",
    "**Average savings per item:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be44696b-54ea-4a27-aec1-8db2890c4606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average savings per item (with substitute): $4.07\n",
      "Average savings per item (overall): $2.99\n",
      "Total savings across all items: $74399.30\n"
     ]
    }
   ],
   "source": [
    "# Ensure suggested_savings is numeric\n",
    "df_results['suggested_savings'] = pd.to_numeric(df_results['suggested_savings'], errors='coerce')\n",
    "\n",
    "# Calculate average and total savings\n",
    "avg_savings = df_results['suggested_savings'].dropna().mean()\n",
    "avg_savings_including_empty = df_results['suggested_savings'].fillna(0).mean()\n",
    "total_savings = df_results['suggested_savings'].dropna().sum()\n",
    "\n",
    "print(f\"Average savings per item (with substitute): ${avg_savings:.2f}\")\n",
    "print(f\"Average savings per item (overall): ${avg_savings_including_empty:.2f}\")\n",
    "print(f\"Total savings across all items: ${total_savings:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348328b7-dc0a-40af-81d9-d01e0d35a4d6",
   "metadata": {},
   "source": [
    "As we can see above, the average savings per item where a substitute was found is \\\\$4.07. We can also check the average savings across all items, including the ones with no suggestion being set to \\\\$0 savings, and this gives an average of \\\\$2.99. Overall, the suggested substitutions save a combined \\\\$74399.30 across all items.\n",
    "\n",
    "**Swap acceptance rate:**\n",
    "\n",
    "Another key metric we can check is what percentage of the suggested items meet our criteria for a successful swap. These criteria can be adjusted based on the nature of the items we went the model to suggest - in this evaluation it checks that there is a suggested item, that the subcategory is the same, that the size band is the same, that it has the same unit type, and that its similarity is above 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3339ccb-3f40-4dc8-9e1f-4b9270cfe85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted swaps: 16610\n",
      "Total swaps suggested: 18274\n",
      "Swap Acceptance Rate (out of suggested items): 90.89%\n",
      "Swap Acceptance Rate (out of all items): 66.71%\n"
     ]
    }
   ],
   "source": [
    "# Define swap acceptance rules\n",
    "# Valid if:\n",
    "# 1. There is a suggested item\n",
    "# 2. Same subcategory\n",
    "# 3. Same size_band\n",
    "# 4. Same unit_type\n",
    "# 5. Similarity above threshold (0.5)\n",
    "valid_swaps_mask = (\n",
    "    df_results['suggested_product_code'].notna() &\n",
    "    (df_results['subcategory'] == df_results['suggested_subcategory']) &\n",
    "    (df_results['size_band'] == df_results['suggested_size_band']) &\n",
    "    (df_results['unit_type'] == df_results['suggested_unit_type']) &\n",
    "    (df_results['suggested_similarity'] >= 0.5)\n",
    ")\n",
    "\n",
    "accepted_swaps = valid_swaps_mask.sum()\n",
    "total_swaps_suggested = df_results['suggested_product_code'].notna().sum()\n",
    "total_items = len(df_results)\n",
    "\n",
    "# Swap acceptance rate out of items that had suggestions\n",
    "acceptance_rate_suggested = accepted_swaps / total_swaps_suggested if total_swaps_suggested > 0 else 0\n",
    "\n",
    "# Swap acceptance rate out of all items\n",
    "acceptance_rate_all = accepted_swaps / total_items if total_items > 0 else 0\n",
    "\n",
    "print(f\"Accepted swaps: {accepted_swaps}\")\n",
    "print(f\"Total swaps suggested: {total_swaps_suggested}\")\n",
    "print(f\"Swap Acceptance Rate (out of suggested items): {acceptance_rate_suggested:.2%}\")\n",
    "print(f\"Swap Acceptance Rate (out of all items): {acceptance_rate_all:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4fdf0-2f2a-4fc9-93f2-071f8272e106",
   "metadata": {},
   "source": [
    "As we can see above, the model has performed really well. Out of the suggested items, 90% of them have been accepted based on our specified conditions. If we want to measure this out of the total number of items, not just those with suggestions, the model achieved a moderate 66%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791d02f-3c19-4705-af5b-45df36d329ac",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Overall the model performed very well. With a swap acceptance rate of 90%, 9/10 of the suggested items met the criteria outlining an effective swap, and this was for the 73% of items that did receive a suggested substitution. Additionally, the swap acceptance rate including items that did not have any item suggested, was still 66%. Out of all the items suggested, the model saved $74399, with an average of \\\\$4.07 per suggested item. This is a significant amount of savings for shoppers, and the swap acceptance rate highlights that these items are similar too, not just cheaper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
