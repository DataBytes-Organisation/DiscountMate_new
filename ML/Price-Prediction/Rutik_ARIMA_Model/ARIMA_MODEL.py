# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ny2g2TPkHy-jOnKSxkktj4FJadYrKhVX
"""

import pandas as pd
import numpy as np

df = pd.read_csv('Synthetic_Products 1.csv')
df.head()

df.info()

# Check for any None or empty values in the DataFrame
if df.isnull().values.any() or df.empty:
  print("DataFrame 'df' contains None or empty values.")
else:
  print("DataFrame 'df' does not contain None or empty values.")

# Fill null values with the mean of the respective columns
for col in df.columns:
    if pd.api.types.is_numeric_dtype(df[col]):
        df[col] = df[col].fillna(df[col].mean())
    elif pd.api.types.is_string_dtype(df[col]):
        df[col] = df[col].fillna(df[col].mode()[0])

df.info()

# List of fields unrelated to the price prediction task
fields_to_remove = [
    "index",
    "Product_Url",
    "Sku",
    "tid"
]

# Drop the unrelated fields from the dataframe
df = df.drop(columns=fields_to_remove)

df.head()

import re

def standardize_price_unit(price_str):
    price_str = str(price_str)
    match = re.search(r"(\$\d+\.?\d*)", price_str)
    if match:
        price = float(match.group(1)[1:])
    else:
        return np.nan  # or handle the case where no price is found appropriately

    unit_match = re.search(r"per\s+(\d+\.?\d*)\s*(kg|Kg|KG|g|gm|GM|Gm|gM|Gm)", price_str, re.IGNORECASE)
    if unit_match:
        quantity = float(unit_match.group(1))
        unit = unit_match.group(2).lower()

        if unit.startswith('kg'):
            return price / quantity
        elif unit.startswith('g'):
            return (price / quantity) * 1000
        else:
            return np.nan # Handle unknown unit
    else:
        return np.nan

df['price_per_kg'] = df['Price_per_unit'].apply(standardize_price_unit)
df.head()

df = df.drop('Price_per_unit', axis=1)
df.head()

df = df.drop(['package_size', 'is_estimated'], axis=1)
df.head()

df.info()

df['RunDate'] = pd.to_datetime(df['RunDate'])
df.info()

import seaborn as sns
import matplotlib.pyplot as plt


plt.figure(figsize=(8, 5))
sns.histplot(df['unit_price'], bins=30, kde=True)
plt.title('Distribution of Retail Prices')
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.show()

# Select only numeric columns
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Calculate the correlation matrix
corr_matrix = numeric_df.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

print(df.isnull().sum())

df['price_per_kg'] = pd.to_numeric(df['price_per_kg'], errors='coerce')  # Ensure it's numeric
df['price_per_kg'].replace(0, pd.NA)  # Replace 0 with NaN

# Drop rows where 'price_per_kg' is NaN
df = df.dropna(subset=['price_per_kg'])

print(df.isnull().sum())

# Export the DataFrame to a CSV file
df.to_csv('cleaned_data.csv', index=False)

"""#Model building"""

from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

# Step 1: Data Preparation
df = pd.read_csv('cleaned_data.csv')
# Ensure 'RunDate' is datetime and data is sorted
df['RunDate'] = pd.to_datetime(df['RunDate'])
df = df.sort_values('RunDate')

print(df['RunDate'].nunique())
print(df['RunDate'].unique()[:5])


# Step 2: Aggregate Data
time_series = df.groupby('RunDate')['price_per_kg'].mean()

time_series.plot()
plt.title('Time Series Plot')
plt.show()


print("------------------------------------------")
print(time_series.describe())
print(time_series.head())
print(df['price_per_kg'].describe())
print(df.groupby('RunDate')['price_per_kg'].mean().head())
print("------------------------------------------")

# Step 3: Check for Stationarity
def check_stationarity(series):
    result = adfuller(series)
    print("ADF Statistic:", result[0])
    print("p-value:", result[1])
    if result[1] <= 0.05:
        print("The time series is stationary.")
    else:
        print("The time series is NOT stationary. Consider differencing.")

check_stationarity(time_series)

# Step 4: Make Series Stationary (if needed)
# Apply differencing if series is not stationary
if adfuller(time_series)[1] > 0.05:
    time_series_diff = time_series.diff().dropna()
else:
    time_series_diff = time_series

# Plot the data
plt.figure(figsize=(10, 6))
plt.plot(time_series, label='Original Time Series')
plt.plot(time_series_diff, label='Differenced Time Series')
plt.legend()
plt.title("Time Series Data")
plt.show()

# Step 5: Plot ACF and PACF to Determine ARIMA Parameters
plot_acf(time_series_diff, lags=20)
plt.show()

plot_pacf(time_series_diff, lags=20)
plt.show()

# Step 6: Fit ARIMA Model
# Using (p, d, q) = (1, 1, 1) as an example; tune this based on ACF/PACF plots
model = ARIMA(time_series, order=(1, 1, 1))  # Adjust (p, d, q) as needed
model_fit = model.fit()

# Print model summary
print(model_fit.summary())

# Step 7: Forecast Future Prices
forecast_steps = 10  # Number of future periods to forecast
forecast = model_fit.forecast(steps=forecast_steps)

# Plot forecast
plt.figure(figsize=(10, 6))
plt.plot(time_series, label='Historical Data')
plt.plot(pd.date_range(start=time_series.index[-1], periods=forecast_steps + 1, freq='D')[1:], forecast, label='Forecast', color='orange')
plt.legend()
plt.title("Price Forecast")
plt.show()

# Step 8: Evaluate Model Performance
# Split data into train and test sets for evaluation
train_size = int(len(time_series) * 0.8)
train, test = time_series[:train_size], time_series[train_size:]

# Fit ARIMA on training data
model_train = ARIMA(train, order=(1, 1, 1))
model_train_fit = model_train.fit()

# Forecast on test data
test_forecast = model_train_fit.forecast(steps=len(test))

# Calculate error
mse = mean_squared_error(test, test_forecast)
print("Mean Squared Error (MSE):", mse)

# Plot actual vs forecasted
plt.figure(figsize=(10, 6))
plt.plot(test.index, test, label='Actual Prices')
plt.plot(test.index, test_forecast, label='Forecasted Prices', color='orange')
plt.legend()
plt.title("Actual vs Forecasted Prices")
plt.show()

"""#Model building with "Synthetic data"
"""

new_data = pd.read_csv('Aus_grocery_synthetic_dataset2.csv')

new_data.info()
new_data.head()

new_data = new_data[~new_data['unit_price_x'].isin([0]) & new_data['unit_price_x'].notna()]

"""# Working on cleaned data"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('Aus_grocery_synthetic_dataset2.csv')
df.head()

print(df.isnull().sum())

"""We should not use any random nunber for missing price values so removing all rows whose "unit_price_x" value is either "0" or "NaN"."""

df = df[(df['unit_price_x'] != 0) & (df['unit_price_x'].notna())]

df.info()

def parse_mixed_dates(date):
    try:
        # Attempt parsing as MM/DD/YYYY
        return pd.to_datetime(date, format='%m/%d/%Y')
    except ValueError:
        try:
            # Attempt parsing as DD/MM/YYYY
            return pd.to_datetime(date, format='%d/%m/%Y')
        except ValueError:
            return pd.NaT  # Return NaT if both fail

# Apply the function to parse mixed formats
df['RunDate_parsed'] = df['RunDate'].apply(parse_mixed_dates)

df.head()

print(df.isnull().sum())

"""#Outliers detection and removal"""

mean = df['unit_price_x'].mean()
std = df['unit_price_x'].std()
df = df[(df['unit_price_x'] >= mean - 3 * std) & (df['unit_price_x'] <= mean + 3 * std)]

df.info()

from statsmodels.tsa.stattools import adfuller

result = adfuller(df['unit_price_x'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])

"""The results of your Augmented Dickey-Fuller (ADF) test indicate the following:

### ADF Test Results:
- **ADF Statistic**: `-36.83`
  - A very low ADF statistic indicates strong evidence against the null hypothesis of non-stationarity.
- **p-value**: `0.0`
  - A p-value of `0.0` (or close to 0) is far below the typical significance level (e.g., 0.05), meaning we reject the null hypothesis of non-stationarity.

---

### Interpretation:
1. **Stationarity**:
   - The time series is **stationary**, meaning it does not have a unit root. Stationarity is a key requirement for ARIMA modeling.

2. **Next Steps**:
   - Since the series is already stationary, you **do not need to difference the series** further.
   - Proceed with fitting the ARIMA model, starting with `d=0` (no differencing).


"""

from statsmodels.tsa.arima.model import ARIMA
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

plot_acf(df['unit_price_x'], lags=20)  # Autocorrelation plot
plot_pacf(df['unit_price_x'], lags=20)  # Partial autocorrelation plot
plt.show()

# Based on ACF/PACF, determine p and q (example: p=1, q=1 for simplicity)
# You can adjust these based on the ACF and PACF plots or domain knowledge

# 4. ARIMA Model fitting
p = 1  # Autoregressive order (p)
d = 0  # Differencing order (d) as series is stationary
q = 1  # Moving average order (q)

# Fit the ARIMA model
model = ARIMA(df['unit_price_x'], order=(p, d, q))
model_fit = model.fit()

# 5. Model Summary
print(model_fit.summary())

# 6. Predict the next 'n' values (forecasting)
n_forecast = 10  # Number of periods to forecast
forecast = model_fit.forecast(steps=n_forecast)

# Plot the forecasted values
plt.plot(df.index, df['unit_price_x'], label='Historical Price')
forecast_index = pd.date_range(start=df.index[-1], periods=n_forecast+1, freq='D')[1:]
plt.plot(forecast_index, forecast, label='Forecasted Price', color='red')

plt.legend()
plt.show()

import numpy as np

# Apply log transformation (add a small constant to avoid log(0))
df['unit_price_x_log'] = np.log(df['unit_price_x'] + 1)

# Plot the transformed data to check the effect
plt.plot(df['unit_price_x'], label='Original')
plt.plot(df['unit_price_x_log'], label='Log Transformed')
plt.legend()
plt.show()

from statsmodels.tsa.stattools import adfuller

result = adfuller(df['unit_price_x_log'])
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')
if result[1] < 0.05:
    print("Log-transformed data is stationary.")
else:
    print("Log-transformed data is not stationary.")

plot_acf(df['unit_price_x_log'], lags=20)
plot_pacf(df['unit_price_x_log'], lags=20)
plt.show()

from statsmodels.tsa.arima.model import ARIMA

# Replace p and q based on ACF/PACF analysis
p = 1
q = 1
model = ARIMA(df['unit_price_x_log'], order=(p, 0, q))
model_fit = model.fit()

# Display model summary
print(model_fit.summary())
n_forecast = 10  # Number of steps to forecast
forecast_log = model_fit.forecast(steps=n_forecast)

# Inverse log transformation
forecast_original = np.exp(forecast_log) - 1

# Plot results
forecast_index = pd.date_range(start=df.index[-1], periods=n_forecast+1, freq='D')[1:]
plt.plot(df.index, df['unit_price_x'], label='Historical Price')
plt.plot(forecast_index, forecast_original, label='Forecasted Price', color='red')
plt.legend()
plt.show()



"""#SARIMAX"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
# Check for missing values
df = df.dropna(subset=['unit_price_x'])

train_size = int(len(df) * 0.8)
train, test = df['unit_price_x'][:train_size], df['unit_price_x'][train_size:]

# SARIMAX model
model = SARIMAX(train,
                order=(1, 0, 1),
                seasonal_order=(1, 1, 1, 12),
                enforce_stationarity=False,
                enforce_invertibility=False)

# Fit the model
model_fit = model.fit(disp=False)

# Print the summary of the model
print(model_fit.summary())

forecast = model_fit.get_forecast(steps=len(test))

# Get confidence intervals
conf_int = forecast.conf_int()

# Plot the results
plt.figure(figsize=(10, 6))
plt.plot(df.index, df['unit_price_x'], label='Historical Price')
plt.plot(test.index, forecast.predicted_mean, color='red', label='Forecasted Price')
plt.fill_between(test.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='pink', alpha=0.3)
plt.legend()
plt.title('SARIMAX Model - Price Forecasting')
plt.show()

"""##Data Loading"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('Aus_grocery_synthetic_dataset2.csv')
df.head()

"""##Removing all rows whose "unit_price_x value is either "0" or "NaN"."""

df = df[(df['unit_price_x'] != 0) & (df['unit_price_x'].notna())]

"""#Transfering date column type to "DateTimeObject"
"""

def parse_mixed_dates(date):
    try:
        # Attempt parsing as MM/DD/YYYY
        return pd.to_datetime(date, format='%m/%d/%Y')
    except ValueError:
        try:
            # Attempt parsing as DD/MM/YYYY
            return pd.to_datetime(date, format='%d/%m/%Y')
        except ValueError:
            return pd.NaT  # Return NaT if both fail

# Apply the function to parse mixed formats
df['RunDate_parsed'] = df['RunDate'].apply(parse_mixed_dates)

df.info()

print(df.describe())

# Plot for unit_price_x
sns.histplot(df['unit_price_x'], kde=True)
plt.title('Distribution of unit_price_x')
plt.show()

"""##Normaliz the "unit_price_x" column"""

from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Fit and transform the unit_price_x column
df['unit_price_x_normalized'] = scaler.fit_transform(df[['unit_price_x']])

# Check the result
# Plot for unit_price_x
sns.histplot(df['unit_price_x_normalized'], kde=True)
plt.title('Distribution of unit_price_x_normalized')
plt.show()

# Apply log transformation
df['unit_price_x_log'] = np.log1p(df['unit_price_x'])

# Check the result
# Plot for unit_price_x
sns.histplot(df['unit_price_x_log'], kde=True)
plt.title('Distribution of unit_price_x_log')
plt.show()

from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA

# Check for stationarity on log-transformed data
price_data_log = df['unit_price_x_log']
result = adfuller(price_data_log)
print('ADF Statistic:', result[0])
print('p-value:', result[1])

# Differencing if necessary
if result[1] > 0.05:
    price_data_log = price_data_log.diff().dropna()

# Split data
train_size = int(len(price_data_log) * 0.8)
train, test = price_data_log[:train_size], price_data_log[train_size:]

# Fit ARIMA model
model = ARIMA(train, order=(5,1,0))
model_fit = model.fit()

# Forecast
forecast = model_fit.forecast(steps=len(test))
forecast_index = test.index

# Plot the results
plt.figure(figsize=(10,5))
plt.plot(train, label='Training Data')
plt.plot(test, label='Testing Data')
plt.plot(forecast_index, forecast, label='Forecast')
plt.legend(loc='best')
plt.title('ARIMA Model Forecast on Log-Transformed Data')
plt.show()

print("ARIMA model summary:")
print(model_fit.summary())

"""# Calculate performance metrics"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

forecast_array = np.array(forecast)
mae = mean_absolute_error(test, forecast_array)
mse = mean_squared_error(test, forecast_array)
rmse = np.sqrt(mse)
r2 = r2_score(test, forecast_array)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-Squared: {r2}")

# Optional: Plot residuals
residuals = model_fit.resid
plt.figure(figsize=(10, 5))
plt.plot(residuals)
plt.title('Residuals of the ARIMA Model')
plt.show()

# ACF/PACF of residuals
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(residuals, lags=40)
plt.show()
plot_pacf(residuals, lags=40)
plt.show()

"""#Model saving"""

import pickle
with open('arima_model_log_normalization.pkl', 'wb') as pkl:
  pickle.dump(model_fit, pkl)

"""#Performing above same steps with "MinMax Scaler" Normalization"""

from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA

# Check for stationarity on log-transformed data
price_data_normalized = df['unit_price_x_normalized']
result = adfuller(price_data_normalized)
print('ADF Statistic:', result[0])
print('p-value:', result[1])

# Differencing if necessary
if result[1] > 0.05:
    price_data_normalized = price_data_normalized.diff().dropna()

# Split data
train_size = int(len(price_data_normalized) * 0.8)
train, test = price_data_normalized[:train_size], price_data_normalized[train_size:]

# Fit ARIMA model
model = ARIMA(train, order=(5,1,0))
model_fit = model.fit()

# Forecast
forecast = model_fit.forecast(steps=len(test))
forecast_index = test.index

# Plot the results
plt.figure(figsize=(10,5))
plt.plot(train, label='Training Data')
plt.plot(test, label='Testing Data')
plt.plot(forecast_index, forecast, label='Forecast')
plt.legend(loc='best')
plt.title('ARIMA Model Forecast on Normalized Data')
plt.show()

print("ARIMA model summary:")
print(model_fit.summary())

"""# Calculate performance metrics"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

forecast_array = np.array(forecast)
mae = mean_absolute_error(test, forecast_array)
mse = mean_squared_error(test, forecast_array)
rmse = np.sqrt(mse)
r2 = r2_score(test, forecast_array)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-Squared: {r2}")

# Optional: Plot residuals
residuals = model_fit.resid
plt.figure(figsize=(10, 5))
plt.plot(residuals)
plt.title('Residuals of the ARIMA Model')
plt.show()

# ACF/PACF of residuals
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(residuals, lags=40)
plt.show()
plot_pacf(residuals, lags=40)
plt.show()

import pickle
with open('arima_model_MinMax_normalization.pkl', 'wb') as pkl:
  pickle.dump(model_fit, pkl)

