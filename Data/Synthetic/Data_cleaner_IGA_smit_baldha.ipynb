{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8016764574530214\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'IGA-STORES.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Process IGA data\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m stores_list \u001b[38;5;241m=\u001b[39m \u001b[43mextract_stores_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIGA-STORES.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m df_iga \u001b[38;5;241m=\u001b[39m clean_and_restructure_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIGA_1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, stores_list)\n\u001b[1;32m    146\u001b[0m df_iga\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIGA_1_updated_new.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m, in \u001b[0;36mextract_stores_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_stores_from_pdf\u001b[39m(pdf_path):\n\u001b[1;32m     24\u001b[0m     stores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpdfplumber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf\u001b[38;5;241m.\u001b[39mpages:\n\u001b[1;32m     27\u001b[0m             tables \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mextract_table()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pdfplumber/pdf.py:98\u001b[0m, in \u001b[0;36mPDF.open\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, unicode_norm, repair, gs_path, repair_setting, raise_unicode_errors)\u001b[0m\n\u001b[1;32m     96\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_fp, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[0;32m---> 98\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_or_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     stream_is_external \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path_or_fp)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'IGA-STORES.pdf'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys\n",
    "\n",
    "# Generate a list of random dates from the last 7 days\n",
    "def random_date():\n",
    "    return (datetime.today() - timedelta(days=random.randint(0, 6))).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Load data from multiple CSV files for training\n",
    "def load_training_data(files):\n",
    "    return pd.concat([pd.read_csv(file)[['Item Name', 'Category']] for file in files], ignore_index=True)\n",
    "\n",
    "# Extract store names and suburbs from PDF\n",
    "def extract_stores_from_pdf(pdf_path):\n",
    "    stores = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            tables = page.extract_table()\n",
    "            if tables:\n",
    "                for row in tables[1:]:\n",
    "                    filtered_row = [cell for cell in row if cell]\n",
    "                    if len(filtered_row) < 3:\n",
    "                        continue\n",
    "                    store_name, location = filtered_row[0], \" \".join(filtered_row[-6:])\n",
    "                    stores.append((store_name, location))\n",
    "    return stores\n",
    "\n",
    "# Train model on Woolworths and Coles data\n",
    "df_train = load_training_data(['woolworths_cleaned.csv', 'Coles.csv'])  \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['Item Name'], df_train['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n",
    "\n",
    "# Function to clean and process IGA dataset\n",
    "def clean_and_restructure_data(file_path, stores_list):\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1').dropna(how='all')\n",
    "    if df['Product Code'].isnull().any():\n",
    "        sys.exit(\"Error: 'Product Code' column contains missing values.\")\n",
    "    \n",
    "    df.rename(columns={\n",
    "        'Product Code': 'item_id',\n",
    "        'Item Name': 'item_name',\n",
    "        'Best Unit Price': 'best_price',\n",
    "        'Unit Price': 'unit_price',\n",
    "        'Best Price': 'total_price', \n",
    "        'Price Was': 'price_was'\n",
    "    }, inplace=True)\n",
    "    df.drop(columns=['Item Price', 'Special Text', 'Link'], inplace=True, errors='ignore')\n",
    "\n",
    "    df['best_price'] = df['best_price'].str.extract(r'(\\d+\\.\\d+)').astype(float)\n",
    "    df['price_was'] = df['price_was'].str.replace('$', '', regex=False).astype(float)\n",
    "    df['discount'] = df['price_was'] - df['best_price']\n",
    "    df['unit_price_numeric'] = df['unit_price'].str.extract(r'(\\d+\\.\\d+)').astype(float)\n",
    "    df['total_price'] = df['total_price'].str.replace('$', '', regex=False).astype(float)\n",
    "\n",
    "    def calculate_quantity(row):\n",
    "        if pd.notnull(row['unit_price_numeric']) and row['unit_price_numeric'] != 0:\n",
    "            if 'each' in str(row['unit_price']):\n",
    "                return round(row['total_price'] / row['unit_price_numeric'], 2), 'each'\n",
    "            elif 'per 100mL' in str(row['unit_price']):\n",
    "                return round(row['total_price'] / row['unit_price_numeric'] / 10, 2), 'L'\n",
    "            elif 'per 100g' in str(row['unit_price']):\n",
    "                return round(row['total_price'] / row['unit_price_numeric'] / 10, 2), 'Kg'\n",
    "            elif ' kg' in str(row['unit_price']):\n",
    "                return round(row['total_price'] / row['unit_price_numeric'], 2), 'Kg'\n",
    "            elif ' L' in str(row['unit_price']):\n",
    "                return round(row['total_price'] / row['unit_price_numeric'], 2), 'L'\n",
    "        return None, None\n",
    "\n",
    "    df[['quantity', 'unit_type']] = df.apply(lambda row: pd.Series(calculate_quantity(row)), axis=1)\n",
    "    df['unit_price'] = df['unit_price_numeric']\n",
    "    df.drop(columns=['unit_price_numeric', 'price_was'], inplace=True)\n",
    "\n",
    "    df[['store_name', 'location']] = pd.DataFrame(random.choices(stores_list, k=len(df)))\n",
    "    df['gender'] = random.choices(['Male', 'Female', 'Others'], k=len(df))\n",
    "    df[\"payment_method\"] = random.choices([\"Cash\", \"Credit Card\", \"Debit Card\", \"Gift Card\", \"Afterpay\"], k=len(df_iga))\n",
    "    df['sub_category'] = ''\n",
    "    df['transaction_id'] = ''\n",
    "    df ['customer_id'] = ''\n",
    "    item_names = df['item_name'].tolist()\n",
    "    brands = []\n",
    "\n",
    "    while item_names:\n",
    "        first_item = item_names[0]\n",
    "        words = first_item.split()\n",
    "        if len(words[0]) != 1 or words[0].isnumeric():\n",
    "            first_word = words[0]\n",
    "        else: \n",
    "            first_word = words[0] + words[1]\n",
    "        matching_items = [item for item in item_names if item.startswith(first_word)]\n",
    "        \n",
    "        if len(matching_items) == 1:\n",
    "            brands.append(first_word)\n",
    "            item_names.remove(matching_items[0])\n",
    "            continue\n",
    "        \n",
    "        second_words = [item.split()[1] for item in matching_items if len(item.split()) > 1]\n",
    "        word_counts = Counter(second_words)\n",
    "        brand_name = [first_word]\n",
    "        last_word_count = len(matching_items)\n",
    "\n",
    "        for i in range(2, max(len(item.split()) for item in matching_items) + 1):\n",
    "            next_words = [item.split()[i - 1] for item in matching_items if len(item.split()) >= i]\n",
    "            next_word_counts = Counter(next_words)\n",
    "            if next_word_counts and max(next_word_counts.values()) == last_word_count:\n",
    "                brand_name.append(max(next_word_counts, key=next_word_counts.get))\n",
    "                last_word_count = next_word_counts[max(next_word_counts, key=next_word_counts.get)]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        full_brand = \" \".join(brand_name)\n",
    "        brands.append(full_brand)\n",
    "        item_names = [item for item in item_names if not item.startswith(full_brand)]\n",
    "        df['brand'] = df['item_name'].apply(lambda x: next((brand for brand in brands if x.startswith(brand)), None))\n",
    "\n",
    "    df['date'] = [random_date() for _ in range(len(df))]\n",
    "    df['category'] = df['item_name'].apply(lambda x: model.predict([x])[0])\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\n",
    "        'item_id', 'item_name', 'brand', 'category', \n",
    "        'best_price', 'unit_price', 'total_price', 'discount',\n",
    "        'quantity', 'unit_type', 'store_name', 'location', 'gender', 'date', 'sub_category', 'transaction_id', 'customer_id'\n",
    "    ]\n",
    "    df = df[column_order]\n",
    "    return df\n",
    "\n",
    "# Process IGA data\n",
    "stores_list = extract_stores_from_pdf(\"IGA-STORES.pdf\")\n",
    "df_iga = clean_and_restructure_data('IGA_1.csv', stores_list)\n",
    "df_iga.to_csv('IGA_1_updated_new.csv', index=False)\n",
    "print(\"Updated IGA dataset saved as 'IGA_1_updated.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myyenv",
   "language": "python",
   "name": "myyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
